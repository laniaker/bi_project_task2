{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c66ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 1: Setup & Config\n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import NotFound\n",
    "\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sorgt daf√ºr, dass Plots im Notebook angezeigt werden\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d68c4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DIAGNOSE & REPARATUR ---\n",
      "‚úÖ QUELLE GEFUNDEN: Dataset 'staging' liegt in Region: 'EU'\n",
      "OK: Ziel-Dataset liegt bereits korrekt in 'EU'.\n",
      "\n",
      "Ready. Bitte jetzt Zelle 3 ausf√ºhren.\n"
     ]
    }
   ],
   "source": [
    "# Zelle 1 & 2: Setup mit automatischer Regionen-Korrektur\n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import NotFound\n",
    "import logging\n",
    "import uuid\n",
    "import datetime\n",
    "\n",
    "# Logging Setup\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# --- CONFIG ---\n",
    "PROJECT_ID = \"taxi-bi-project\" # Deine ID aus dem Log\n",
    "SOURCE_DATASET = \"staging\"     # <--- Laut deinem Fehlerlog hei√üt es \"staging\"!\n",
    "TARGET_DATASET = \"canonical\"   \n",
    "\n",
    "# Tabellen\n",
    "TARGET_TABLE = \"canonical_unified_taxi\"\n",
    "ERROR_TABLE = \"error_records\"\n",
    "LOG_TABLE = \"etl_process_log\"\n",
    "\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "print(\"--- DIAGNOSE & REPARATUR ---\")\n",
    "try:\n",
    "    src_ds_ref = client.get_dataset(f\"{PROJECT_ID}.{SOURCE_DATASET}\")\n",
    "    CORRECT_LOCATION = src_ds_ref.location\n",
    "    print(f\"‚úÖ QUELLE GEFUNDEN: Dataset '{SOURCE_DATASET}' liegt in Region: '{CORRECT_LOCATION}'\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå KRITISCHER FEHLER: Konnte Quell-Dataset '{SOURCE_DATASET}' nicht finden!\")\n",
    "    print(\"Bitte pr√ºfe: Hei√üt dein Dataset in BigQuery wirklich 'staging' oder 'taxi_dwh'?\")\n",
    "    raise e\n",
    "\n",
    "# 2. Pr√ºfen, ob das Ziel-Dataset 'canonical' falsch liegt\n",
    "target_dataset_id = f\"{PROJECT_ID}.{TARGET_DATASET}\"\n",
    "try:\n",
    "    tgt_ds = client.get_dataset(target_dataset_id)\n",
    "    if tgt_ds.location != CORRECT_LOCATION:\n",
    "        print(f\"‚ö†Ô∏è KONFLIKT: Ziel '{TARGET_DATASET}' ist in '{tgt_ds.location}', muss aber nach '{CORRECT_LOCATION}'.\")\n",
    "        print(\"   L√∂sche falsches Dataset...\")\n",
    "        client.delete_dataset(target_dataset_id, delete_contents=True, not_found_ok=True)\n",
    "        print(\"   Gel√∂scht. Wird neu erstellt.\")\n",
    "        tgt_ds = None\n",
    "except NotFound:\n",
    "    tgt_ds = None\n",
    "\n",
    "# 3. Ziel-Dataset korrekt neu erstellen\n",
    "if not tgt_ds:\n",
    "    new_ds = bigquery.Dataset(target_dataset_id)\n",
    "    new_ds.location = CORRECT_LOCATION # <--- Hier zwingen wir die richtige Region!\n",
    "    client.create_dataset(new_ds)\n",
    "    print(f\"ZIEL ERSTELLT: Dataset '{TARGET_DATASET}' erfolgreich in Region '{CORRECT_LOCATION}' angelegt.\")\n",
    "else:\n",
    "    print(f\"OK: Ziel-Dataset liegt bereits korrekt in '{CORRECT_LOCATION}'.\")\n",
    "\n",
    "# Globale Variablen aktualisieren\n",
    "table_ref = f\"{PROJECT_ID}.{TARGET_DATASET}.{TARGET_TABLE}\"\n",
    "error_table_ref = f\"{PROJECT_ID}.{TARGET_DATASET}.{ERROR_TABLE}\"\n",
    "log_table_ref = f\"{PROJECT_ID}.{TARGET_DATASET}.{LOG_TABLE}\"\n",
    "\n",
    "print(\"\\nReady. Bitte jetzt Zelle 3 ausf√ºhren.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb071641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üîç DIAGNOSE: Fehlen Location IDs bei alten Yellow Trips? ---\n",
      " year  total_trips  missing_pu_ids  missing_do_ids  pct_missing\n",
      " 2010     14688619               0               0          0.0\n",
      " 2011     14743286               0               0          0.0\n",
      " 2012     14975598               0               0          0.0\n",
      " 2013     14264747               0               0          0.0\n",
      " 2014     13695497               0               0          0.0\n",
      " 2015     12298345               0               0          0.0\n",
      " 2016     11110224               0               0          0.0\n",
      " 2017      9637812               0               0          0.0\n",
      " 2018      8625756               0               0          0.0\n",
      " 2019      6811024               0               0          0.0\n",
      " 2020       527358               0               0          0.0\n",
      " 2021      2744022               0               0          0.0\n",
      " 2022      3452058               0               0          0.0\n",
      " 2023     37152495               0               0          0.0\n",
      " 2024      3418019               0               0          0.0\n",
      " 2025      3921425               0               0          0.0\n",
      "\n",
      "‚úÖ Entwarnung: Die Staging-Tabelle scheint das schon bereinigt zu haben.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lania/Documents/GitHub/bi_project_task2/.venv/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def check_missing_locations_by_year():\n",
    "    print(\"--- üîç DIAGNOSE: Fehlen Location IDs bei alten Yellow Trips? ---\")\n",
    "    \n",
    "    # Wir gruppieren nach Jahr und z√§hlen, wie viele IDs fehlen\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        EXTRACT(YEAR FROM pickup_datetime) as year,\n",
    "        COUNT(*) as total_trips,\n",
    "        COUNTIF(pickup_location_id IS NULL OR pickup_location_id = 0) as missing_pu_ids,\n",
    "        COUNTIF(dropoff_location_id IS NULL OR dropoff_location_id = 0) as missing_do_ids,\n",
    "        \n",
    "        -- Prozentualer Anteil der Fehler\n",
    "        ROUND(COUNTIF(pickup_location_id IS NULL OR pickup_location_id = 0) / COUNT(*) * 100, 2) as pct_missing\n",
    "        \n",
    "    FROM `{table_ref}`  -- Das ist deine 'canonical_unified_taxi'\n",
    "    WHERE source_system = 'YELLOW'\n",
    "    GROUP BY 1\n",
    "    ORDER BY 1\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        df = client.query(query).to_dataframe()\n",
    "        print(df.to_string(index=False))\n",
    "        \n",
    "        # Sofortige Analyse\n",
    "        if df['pct_missing'].max() > 10:\n",
    "            print(\"\\n‚ö†Ô∏è ALARM: Wir haben signifikante L√ºcken bei den Location IDs!\")\n",
    "            print(\"   -> Wahrscheinlich m√ºssen wir Longitude/Latitude mappen.\")\n",
    "        else:\n",
    "            print(\"\\n‚úÖ Entwarnung: Die Staging-Tabelle scheint das schon bereinigt zu haben.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fehler bei der Diagnose: {e}\")\n",
    "\n",
    "check_missing_locations_by_year()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a050b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üïµÔ∏è‚Äç‚ôÇÔ∏è STAGING INSPECTION (Yellow Taxi) ---\n",
      "Gefundene Spalten in 'taxi-bi-project.staging.yellow_staging_unified':\n",
      " - Airport_fee\n",
      " - DOLocationID\n",
      " - PULocationID\n",
      " - RatecodeID\n",
      " - VendorID\n",
      " - congestion_surcharge\n",
      " - dropoff_latitude\n",
      " - dropoff_longitude\n",
      " - duplicate_flag\n",
      " - extra\n",
      " - fare_amount\n",
      " - improvement_surcharge\n",
      " - missing_flag\n",
      " - mta_tax\n",
      " - passenger_count\n",
      " - payment_type\n",
      " - pickup_latitude\n",
      " - pickup_longitude\n",
      " - store_and_fwd_flag\n",
      " - tip_amount\n",
      " - tolls_amount\n",
      " - total_amount\n",
      " - tpep_dropoff_datetime\n",
      " - tpep_pickup_datetime\n",
      " - trip_distance\n",
      "\n",
      "üîç Koordinaten-Check:\n",
      "‚úÖ Koordinaten gefunden: ['dropoff_latitude', 'dropoff_longitude', 'pickup_latitude', 'pickup_longitude']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lania/Documents/GitHub/bi_project_task2/.venv/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def inspect_staging_columns():\n",
    "    print(\"--- üïµÔ∏è‚Äç‚ôÇÔ∏è STAGING INSPECTION (Yellow Taxi) ---\")\n",
    "    \n",
    "    # Wir schauen uns eine Zeile aus der Staging Tabelle an\n",
    "    # Stelle sicher, dass der Tabellenname stimmt (yellow_staging_unified)\n",
    "    table_id = f\"{PROJECT_ID}.{SOURCE_DATASET}.yellow_staging_unified\"\n",
    "    \n",
    "    try:\n",
    "        # Wir laden nur die Spaltennamen\n",
    "        df = client.query(f\"SELECT * FROM `{table_id}` LIMIT 1\").to_dataframe()\n",
    "        \n",
    "        print(f\"Gefundene Spalten in '{table_id}':\")\n",
    "        cols = sorted(df.columns)\n",
    "        for c in cols:\n",
    "            print(f\" - {c}\")\n",
    "            \n",
    "        # Spezifischer Check auf Koordinaten\n",
    "        print(\"Check:\")\n",
    "        coord_cols = [c for c in cols if 'lat' in c.lower() or 'lon' in c.lower()]\n",
    "        if coord_cols:\n",
    "            print(f\"Koordinaten gefunden: {coord_cols}\")\n",
    "        else:\n",
    "            print(\"Keine direkten Koordinaten-Spalten (Lat/Lon) gefunden.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Lesen von Staging: {e}\")\n",
    "\n",
    "inspect_staging_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf5f2caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabelle neu erstellt: canonical_unified_taxi (Partition: MONTH)\n",
      "Tabelle neu erstellt: error_records (Partition: DAY)\n"
     ]
    }
   ],
   "source": [
    "# Zelle 2: Schema Definition (Fix: MONTH Partitioning)\n",
    "def create_all_tables():\n",
    "    base_schema = [\n",
    "        bigquery.SchemaField(\"trip_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"source_system\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"load_date\", \"TIMESTAMP\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"vendor_id\", \"STRING\"), \n",
    "        bigquery.SchemaField(\"Affiliated_base_number\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"dispatching_base_nummer\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"pickup_datetime\", \"TIMESTAMP\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"dropoff_datetime\", \"TIMESTAMP\"),\n",
    "        bigquery.SchemaField(\"pickup_location_id\", \"INT64\"), \n",
    "        bigquery.SchemaField(\"dropoff_location_id\", \"INT64\"), \n",
    "        bigquery.SchemaField(\"passenger_count\", \"INT64\"),\n",
    "        bigquery.SchemaField(\"trip_distance\", \"FLOAT64\"),\n",
    "        bigquery.SchemaField(\"store_and_fwd_flag\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"RatecodeID\", \"INT64\"),\n",
    "        bigquery.SchemaField(\"Trip_type\", \"INT64\"),\n",
    "        bigquery.SchemaField(\"SR_Flag\", \"BOOLEAN\"), \n",
    "        bigquery.SchemaField(\"fare_amount\", \"FLOAT64\"),\n",
    "        bigquery.SchemaField(\"tip_amount\", \"FLOAT64\"),\n",
    "        bigquery.SchemaField(\"total_amount\", \"FLOAT64\"),\n",
    "        bigquery.SchemaField(\"payment_type\", \"INT64\"),\n",
    "        bigquery.SchemaField(\"extra\", \"FLOAT64\"),\n",
    "        bigquery.SchemaField(\"mta_tax\", \"FLOAT64\"),\n",
    "        bigquery.SchemaField(\"tolls_amount\", \"FLOAT64\"),\n",
    "        bigquery.SchemaField(\"improvement_surcharge\", \"FLOAT64\"),\n",
    "        bigquery.SchemaField(\"congestion_surcharge\", \"FLOAT64\"),\n",
    "        bigquery.SchemaField(\"Airport_fee\", \"FLOAT64\"), \n",
    "        bigquery.SchemaField(\"ehail_fee\", \"FLOAT64\"),   \n",
    "        bigquery.SchemaField(\"dq_issue_flag\", \"BOOLEAN\") \n",
    "    ]\n",
    "\n",
    "    error_schema = base_schema + [bigquery.SchemaField(\"rejection_reason\", \"STRING\")]\n",
    "\n",
    "    # Hier setzen wir explizit MONTH statt DAY\n",
    "    tables_to_create = [\n",
    "        (table_ref, base_schema, \"pickup_datetime\", bigquery.TimePartitioningType.MONTH),\n",
    "        (error_table_ref, error_schema, \"load_date\", bigquery.TimePartitioningType.DAY) # Error bleibt DAY, da load_date nur HEUTE ist\n",
    "    ]\n",
    "\n",
    "    for t_ref, t_schema, p_field, p_type in tables_to_create:\n",
    "        try:\n",
    "            client.delete_table(t_ref, not_found_ok=True)\n",
    "            t = bigquery.Table(t_ref, schema=t_schema)\n",
    "            \n",
    "            t.time_partitioning = bigquery.TimePartitioning(\n",
    "                field=p_field,\n",
    "                type_=p_type\n",
    "            )\n",
    "            \n",
    "            t.clustering_fields = [\"source_system\", \"vendor_id\"]\n",
    "            client.create_table(t)\n",
    "            print(f\"Tabelle neu erstellt: {t_ref.split('.')[-1]} (Partition: {p_type})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler bei {t_ref}: {e}\")\n",
    "\n",
    "create_all_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9ea5df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starte finale ETL Pipeline mit Dubletten-Erkennung...\n",
      " ETL Job erfolgreich abgeschlossen.\n",
      "   - Dubletten werden jetzt erkannt und in die Error-Tabelle verschoben.\n"
     ]
    }
   ],
   "source": [
    "def run_etl_split_logic():\n",
    "    print(f\"üöÄ Starte finale ETL Pipeline mit Dubletten-Erkennung...\")\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    BEGIN\n",
    "        -- 1. TRANSFORMATION IN TEMP-SPEICHER\n",
    "        CREATE OR REPLACE TEMP TABLE temp_trips_processed AS\n",
    "        -- Definition der NYC Grenze als Auffangnetz\n",
    "        WITH nyc_boundary AS (\n",
    "            SELECT ST_UNION_AGG(zone_geom) as city_limit \n",
    "            FROM `{PROJECT_ID}.{SOURCE_DATASET}.taxi_zones_geo`\n",
    "        ),\n",
    "        combined_raw AS (\n",
    "            -- (A) YELLOW\n",
    "            SELECT\n",
    "                'YELLOW' as src,\n",
    "                -- Bereinigte VendorID\n",
    "                CASE\n",
    "                    -- 1. Creative Mobile (CMT) -> 1\n",
    "                    WHEN UPPER(CAST(VendorID AS STRING)) IN ('CMT', '1') THEN '1'\n",
    "                    \n",
    "                    -- 2. VeriFone (VTS/Curb) -> 2\n",
    "                    WHEN UPPER(CAST(VendorID AS STRING)) IN ('VTS', '2', 'VERIFONE', 'CURB') THEN '2'\n",
    "                    \n",
    "                    -- 3. Myle -> 6\n",
    "                    WHEN CAST(VendorID AS STRING) = '6' THEN '6'\n",
    "                    \n",
    "                    -- 4. Helix -> 7\n",
    "                    WHEN CAST(VendorID AS STRING) = '7' THEN '7'\n",
    "                    \n",
    "                    -- 5. Digital Dispatch (DDS) & Unknown / Rest -> 99\n",
    "                    WHEN UPPER(CAST(VendorID AS STRING)) = 'DDS' THEN '99'\n",
    "                    ELSE '99'\n",
    "                END as vid,\n",
    "                -- Zeitstempel\n",
    "                COALESCE(SAFE_CAST(tpep_pickup_datetime AS TIMESTAMP), SAFE.PARSE_TIMESTAMP('%Y-%m-%d %H:%M:%S', CAST(tpep_pickup_datetime AS STRING))) as t_pick,\n",
    "                COALESCE(SAFE_CAST(tpep_dropoff_datetime AS TIMESTAMP), SAFE.PARSE_TIMESTAMP('%Y-%m-%d %H:%M:%S', CAST(tpep_dropoff_datetime AS STRING))) as t_drop,\n",
    "                -- Location Pickup mit NYC-Boundary Logik\n",
    "                CASE \n",
    "                    WHEN CAST(PULocationID AS INT64) BETWEEN 1 AND 263 THEN CAST(PULocationID AS INT64)\n",
    "                    WHEN pickup_longitude != 0 AND pickup_latitude != 0 THEN \n",
    "                        COALESCE(\n",
    "                            (SELECT ANY_VALUE(location_id) FROM `{PROJECT_ID}.{SOURCE_DATASET}.taxi_zones_geo` \n",
    "                             WHERE ST_WITHIN(SAFE.ST_GEOGPOINT(pickup_longitude, pickup_latitude), zone_geom)),\n",
    "                            IF(ST_WITHIN(SAFE.ST_GEOGPOINT(pickup_longitude, pickup_latitude), (SELECT city_limit FROM nyc_boundary)), 264, 265)\n",
    "                        )\n",
    "                    ELSE 264 \n",
    "                END as loc_pu,\n",
    "                -- Location Dropoff mit NYC-Boundary Logik\n",
    "                CASE \n",
    "                    WHEN CAST(DOLocationID AS INT64) BETWEEN 1 AND 263 THEN CAST(DOLocationID AS INT64)\n",
    "                    WHEN dropoff_longitude != 0 AND dropoff_latitude != 0 THEN \n",
    "                        COALESCE(\n",
    "                            (SELECT ANY_VALUE(location_id) FROM `{PROJECT_ID}.{SOURCE_DATASET}.taxi_zones_geo` \n",
    "                             WHERE ST_WITHIN(SAFE.ST_GEOGPOINT(dropoff_longitude, dropoff_latitude), zone_geom)),\n",
    "                            IF(ST_WITHIN(SAFE.ST_GEOGPOINT(dropoff_longitude, dropoff_latitude), (SELECT city_limit FROM nyc_boundary)), 264, 265)\n",
    "                        )\n",
    "                    ELSE 264 \n",
    "                END as loc_do,\n",
    "                IFNULL(CAST(passenger_count AS INT64), 1) as pax,\n",
    "                CAST(trip_distance AS FLOAT64) as dist,\n",
    "                IFNULL(CAST(store_and_fwd_flag AS STRING), 'N') as flag,\n",
    "                COALESCE(SAFE_CAST(RatecodeID AS INT64), 99) as rate,\n",
    "                1 as t_type,\n",
    "                CAST(NULL AS BOOL) as sr_flag,\n",
    "                GREATEST(IFNULL(CAST(fare_amount AS FLOAT64), 0), 0) as f_amt,\n",
    "                GREATEST(IFNULL(CAST(tip_amount AS FLOAT64), 0), 0) as t_amt,\n",
    "                GREATEST(IFNULL(CAST(total_amount AS FLOAT64), 0), 0) as tot_amt,\n",
    "                CASE\n",
    "                    WHEN LOWER(CAST(payment_type AS STRING)) IN ('cre', 'credit', '1') THEN 1\n",
    "                    WHEN LOWER(CAST(payment_type AS STRING)) IN ('cas', 'cash', '2') THEN 2\n",
    "                    ELSE 5\n",
    "                END as pay,\n",
    "                GREATEST(IFNULL(CAST(extra AS FLOAT64), 0), 0) as ex,\n",
    "                GREATEST(IFNULL(CAST(mta_tax AS FLOAT64), 0), 0) as mt,\n",
    "                GREATEST(IFNULL(CAST(tolls_amount AS FLOAT64), 0), 0) as tl,\n",
    "                GREATEST(IFNULL(CAST(improvement_surcharge AS FLOAT64), 0), 0) as im,\n",
    "                GREATEST(IFNULL(CAST(congestion_surcharge AS FLOAT64), 0), 0) as co,\n",
    "                GREATEST(IFNULL(CAST(Airport_fee AS FLOAT64), 0), 0) as ai,\n",
    "                CAST(NULL AS FLOAT64) as eh,\n",
    "                CAST(NULL AS STRING) as aff,\n",
    "                CAST(NULL AS STRING) as disp\n",
    "            FROM `{PROJECT_ID}.{SOURCE_DATASET}.yellow_staging_unified`\n",
    "            WHERE (EXTRACT(YEAR FROM tpep_pickup_datetime) = 2023)\n",
    "               OR (EXTRACT(MONTH FROM tpep_pickup_datetime) = 6)\n",
    "\n",
    "            UNION ALL\n",
    "           -- (B) GREEN\n",
    "            SELECT \n",
    "                'GREEN' as src, \n",
    "                -- Bereinigte VendorID\n",
    "                CASE\n",
    "                    -- 1. Creative Mobile (CMT) -> 1\n",
    "                    WHEN UPPER(CAST(VendorID AS STRING)) IN ('CMT', '1') THEN '1'\n",
    "                    \n",
    "                    -- 2. VeriFone (VTS/Curb) -> 2\n",
    "                    WHEN UPPER(CAST(VendorID AS STRING)) IN ('VTS', '2', 'VERIFONE', 'CURB') THEN '2'\n",
    "                    \n",
    "                    -- 3. Myle -> 6\n",
    "                    WHEN CAST(VendorID AS STRING) = '6' THEN '6'\n",
    "                    \n",
    "                    -- 4. Helix -> 7\n",
    "                    WHEN CAST(VendorID AS STRING) = '7' THEN '7'\n",
    "                    \n",
    "                    -- 5. Digital Dispatch (DDS) & Unknown / Rest -> 99\n",
    "                    WHEN UPPER(CAST(VendorID AS STRING)) = 'DDS' THEN '99'\n",
    "                    ELSE '99'\n",
    "                END as vid,\n",
    "                CAST(lpep_pickup_datetime AS TIMESTAMP) as t_pick, \n",
    "                CAST(lpep_dropoff_datetime AS TIMESTAMP) as t_drop,\n",
    "                CAST(IFNULL(PULocationID, 263) AS INT64) as loc_pu, \n",
    "                CAST(IFNULL(DOLocationID, 263) AS INT64) as loc_do,\n",
    "                CAST(passenger_count AS INT64) as pax, \n",
    "                CAST(trip_distance AS FLOAT64) as dist, \n",
    "                store_and_fwd_flag as flag,\n",
    "                CAST(RatecodeID AS INT64) as rate, \n",
    "                CAST(trip_type AS INT64) as t_type, \n",
    "                FALSE as sr_flag,\n",
    "                fare_amount as f_amt, \n",
    "                tip_amount as t_amt, \n",
    "                total_amount as tot_amt, \n",
    "                CASE \n",
    "                    WHEN payment_type IS NOT NULL THEN CAST(ROUND(SAFE_CAST(payment_type AS FLOAT64)) AS INT64)\n",
    "                    WHEN payment_type IS NULL AND fare_amount > 0 THEN 5 \n",
    "                    ELSE 0 \n",
    "                END AS pay,\n",
    "                extra as ex, mta_tax as mt, tolls_amount as tl, improvement_surcharge as im, \n",
    "                congestion_surcharge as co, 0.0 as ai, ehail_fee as eh, CAST(NULL AS STRING) as aff, CAST(NULL AS STRING) as disp\n",
    "            FROM `{PROJECT_ID}.{SOURCE_DATASET}.green_staging_unified`\n",
    "            WHERE EXTRACT(YEAR FROM lpep_pickup_datetime) >= 2015\n",
    "\n",
    "            UNION ALL\n",
    "\n",
    "            -- (C) FHV\n",
    "            SELECT \n",
    "                'FHV' as src, \n",
    "                '99' as vid, -- Numerische VendorID ist f√ºr FHV immer 99\n",
    "                CAST(pickup_datetime AS TIMESTAMP) as t_pick, \n",
    "                CAST(dropOff_datetime AS TIMESTAMP) as t_drop,\n",
    "                \n",
    "                -- Hier kommt deine neue Geo-Logik (PULocationID) rein, falls du sie nutzt:\n",
    "                CAST(IFNULL(PULocationID, 264) AS INT64) as loc_pu, \n",
    "                CAST(IFNULL(DOLocationID, 264) AS INT64) as loc_do,\n",
    "                \n",
    "                NULL as pax, NULL as dist, 'N' as flag, 99 as rate, 2 as t_type, \n",
    "                CASE WHEN CAST(SR_Flag AS STRING) = '1' THEN TRUE ELSE FALSE END as sr_flag,\n",
    "                NULL as f_amt, NULL as t_amt, NULL as tot_amt, 0 as pay,\n",
    "                NULL as ex, NULL as mt, NULL as tl, NULL as im, NULL as co, NULL as ai, NULL as eh,\n",
    "                \n",
    "                -- REINIGUNG DER AFFILIATED BASE NUMBER\n",
    "                CASE \n",
    "                    WHEN Affiliated_base_number IS NULL OR TRIM(Affiliated_base_number) = '' THEN 'UNKNOWN'\n",
    "                    -- Wir extrahieren nur die Zahlen und bauen das 'B' + 5 Stellen (LPAD) neu\n",
    "                    ELSE CONCAT('B', LPAD(REGEXP_EXTRACT(TRIM(Affiliated_base_number), r'[0-9]+'), 5, '0'))\n",
    "                END as aff,\n",
    "\n",
    "                -- REINIGUNG DER DISPATCHING BASE NUMBER\n",
    "                CASE \n",
    "                    WHEN dispatching_base_num IS NULL OR TRIM(dispatching_base_num) = '' THEN 'UNKNOWN'\n",
    "                    -- Gleiche Logik: Zahlen suchen, auf 5 Stellen auff√ºllen, 'B' davor\n",
    "                    ELSE CONCAT('B', LPAD(REGEXP_EXTRACT(TRIM(dispatching_base_num), r'[0-9]+'), 5, '0'))\n",
    "                END as disp\n",
    "\n",
    "            FROM `{PROJECT_ID}.{SOURCE_DATASET}.fhv_staging_unified`\n",
    "            WHERE EXTRACT(YEAR FROM pickup_datetime) >= 2015\n",
    "        ),\n",
    "        numbered_records AS (\n",
    "            SELECT \n",
    "                *,\n",
    "                -- Hier definieren wir die Dubletten-Regel\n",
    "                ROW_NUMBER() OVER (\n",
    "                    PARTITION BY src, vid, t_pick, t_drop, loc_pu, loc_do \n",
    "                    ORDER BY t_pick\n",
    "                ) as row_num\n",
    "            FROM combined_raw\n",
    "        )\n",
    "        SELECT \n",
    "            *,\n",
    "            -- Logik-Erweiterung: Dubletten werden markiert, falls row_num > 1\n",
    "            CASE \n",
    "                WHEN row_num > 1 THEN 'DUPLICATE_RECORD'\n",
    "                WHEN t_pick IS NULL OR t_drop IS NULL THEN 'Incorrect: Missing Timestamps'\n",
    "                WHEN t_pick >= t_drop THEN 'Incorrect: Invalid Duration'\n",
    "                WHEN t_pick > CURRENT_TIMESTAMP() THEN 'Incorrect: Future Date'\n",
    "                WHEN src IN ('YELLOW', 'GREEN') AND (tot_amt <= 0 OR f_amt <= 0) THEN 'Incorrect: Financials'\n",
    "                WHEN pax < 1 OR pax > 6 THEN 'Incorrect: Invalid Pax Count (Rule 1.4)'\n",
    "                WHEN dist < 0 OR dist >= 1000 THEN 'Incorrect: Invalid Distance (Rule 1.5)'\n",
    "                ELSE 'VALID'\n",
    "            END as row_status,\n",
    "            CASE WHEN dist > 500 OR (pay = 2 AND t_amt = 0) THEN TRUE ELSE FALSE END as dq_issue_flag\n",
    "        FROM numbered_records;\n",
    "\n",
    "        -- 2. INSERT VALID DATA (Nur row_status = 'VALID' und row_num = 1)\n",
    "        INSERT INTO `{table_ref}` (\n",
    "            trip_id, source_system, load_date, vendor_id, \n",
    "            Affiliated_base_number, dispatching_base_nummer, \n",
    "            pickup_datetime, dropoff_datetime, pickup_location_id, dropoff_location_id, \n",
    "            passenger_count, trip_distance, store_and_fwd_flag, \n",
    "            RatecodeID, Trip_type, SR_Flag, fare_amount, tip_amount, total_amount, \n",
    "            payment_type, extra, mta_tax, tolls_amount, improvement_surcharge, \n",
    "            congestion_surcharge, Airport_fee, ehail_fee, dq_issue_flag\n",
    "        )\n",
    "        SELECT \n",
    "            CAST(FARM_FINGERPRINT(CONCAT(src, CAST(t_pick AS STRING), IFNULL(vid,''))) AS STRING),\n",
    "            src, CURRENT_TIMESTAMP(), vid, aff, disp, t_pick, t_drop, loc_pu, loc_do, \n",
    "            pax, dist, flag, rate, t_type, sr_flag, f_amt, t_amt, tot_amt, \n",
    "            pay, ex, mt, tl, im, co, ai, eh, dq_issue_flag\n",
    "        FROM temp_trips_processed \n",
    "        WHERE row_status = 'VALID';\n",
    "\n",
    "        -- 3. INSERT ERROR DATA (Alles was nicht VALID ist -> Inklusive Dubletten!)\n",
    "        INSERT INTO `{error_table_ref}` (\n",
    "            trip_id, source_system, load_date, vendor_id, \n",
    "            Affiliated_base_number, dispatching_base_nummer, \n",
    "            pickup_datetime, dropoff_datetime, pickup_location_id, dropoff_location_id, \n",
    "            passenger_count, trip_distance, store_and_fwd_flag, \n",
    "            RatecodeID, Trip_type, SR_Flag, fare_amount, tip_amount, total_amount, \n",
    "            payment_type, extra, mta_tax, tolls_amount, improvement_surcharge, \n",
    "            congestion_surcharge, Airport_fee, ehail_fee, dq_issue_flag, rejection_reason\n",
    "        )\n",
    "        SELECT \n",
    "            CAST(FARM_FINGERPRINT(CONCAT(src, CAST(t_pick AS STRING), IFNULL(vid,''))) AS STRING),\n",
    "            src, CURRENT_TIMESTAMP(), vid, aff, disp, t_pick, t_drop, loc_pu, loc_do, \n",
    "            pax, dist, flag, rate, t_type, sr_flag, f_amt, t_amt, tot_amt, \n",
    "            pay, ex, mt, tl, im, co, ai, eh, TRUE, row_status\n",
    "        FROM temp_trips_processed \n",
    "        WHERE row_status != 'VALID';\n",
    "    END;\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        client.query(query).result()\n",
    "        print(\" ETL Job erfolgreich abgeschlossen.\")\n",
    "        print(\"   - Dubletten werden jetzt erkannt und in die Error-Tabelle verschoben.\")\n",
    "    except Exception as e:\n",
    "        print(f\" Fehler: {e}\")\n",
    "\n",
    "run_etl_split_logic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a7e58dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CLEAN DATA SAMPLE ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lania/Documents/GitHub/bi_project_task2/.venv/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            0\n",
      "monat                       6\n",
      "anzahl               14697820\n",
      "erfolgreich_gemappt  14448322\n",
      "unbekannte_zone        249498\n",
      "\n",
      "--- ERROR DATA SAMPLE  ---\n",
      "                          rejection_reason        cnt\n",
      "0                   Incorrect: Future Date          8\n",
      "1                    Incorrect: Financials     833754\n",
      "2  Incorrect: Invalid Pax Count (Rule 1.4)    1291795\n",
      "3                         DUPLICATE_RECORD   86747863\n",
      "4              Incorrect: Invalid Duration  191679519\n",
      "5   Incorrect: Invalid Distance (Rule 1.5)      24764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lania/Documents/GitHub/bi_project_task2/.venv/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Zelle 4: Quality Check\n",
    "print(\"--- CLEAN DATA SAMPLE ---\")\n",
    "print(client.query(f\"SELECT EXTRACT(MONTH FROM pickup_datetime) as monat, COUNT(*) as anzahl, COUNTIF(pickup_location_id IS NOT NULL AND pickup_location_id != 263) as erfolgreich_gemappt, COUNTIF(pickup_location_id = 263) as unbekannte_zone FROM `{table_ref}` WHERE EXTRACT(YEAR FROM pickup_datetime) = 2010 GROUP BY 1;\").to_dataframe().T)\n",
    "\n",
    "print(\"\\n--- ERROR DATA SAMPLE  ---\")\n",
    "try:\n",
    "    err_df = client.query(f\"SELECT rejection_reason, count(*) as cnt FROM `{error_table_ref}` GROUP BY 1\").to_dataframe()\n",
    "    print(err_df)\n",
    "except:\n",
    "    print(\"Keine Fehler gefunden (Tabelle leer).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62bacbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Starte Daten-Audit (Staging vs. Canonical vs. Error)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lania/Documents/GitHub/bi_project_task2/.venv/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 1. ROHDATEN IN STAGING (gefiltert) ---\n",
      "      src     anzahl\n",
      "0   GREEN   68044817\n",
      "1  YELLOW  178437711\n",
      "2     FHV  783688849\n",
      "Gesamt Rohdaten: 1,030,171,377\n",
      "\n",
      "--- 2. VERTEILUNG IM CANONICAL LAYER ---\n",
      "      src    cat     anzahl\n",
      "0  YELLOW  ERROR    6372729\n",
      "1  YELLOW  VALID  172064982\n",
      "2   GREEN  ERROR     596760\n",
      "3   GREEN  VALID   67448057\n",
      "4     FHV  ERROR  273608214\n",
      "5     FHV  VALID  510080635\n",
      "Gesamt (Valid + Error): 1,030,171,377\n",
      "\n",
      "--- ANALYSE ---\n",
      "Differenz: 0 Zeilen\n",
      " Perfekt: Jede Zeile wurde entweder als VALID oder ERROR verarbeitet.\n"
     ]
    }
   ],
   "source": [
    "def run_data_audit():\n",
    "    print(\"üìä Starte Daten-Audit (Staging vs. Canonical vs. Error)...\")\n",
    "    \n",
    "    # Query f√ºr die Rohdaten (Staging) mit deinen Filtern\n",
    "    query_staging = f\"\"\"\n",
    "    SELECT 'YELLOW' as src, COUNT(*) as anzahl FROM `{PROJECT_ID}.{SOURCE_DATASET}.yellow_staging_unified` \n",
    "    WHERE (EXTRACT(YEAR FROM tpep_pickup_datetime) = 2023) OR (EXTRACT(MONTH FROM tpep_pickup_datetime) = 6)\n",
    "    UNION ALL\n",
    "    SELECT 'GREEN', COUNT(*) FROM `{PROJECT_ID}.{SOURCE_DATASET}.green_staging_unified` WHERE EXTRACT(YEAR FROM lpep_pickup_datetime) >= 2015\n",
    "    UNION ALL\n",
    "    SELECT 'FHV', COUNT(*) FROM `{PROJECT_ID}.{SOURCE_DATASET}.fhv_staging_unified` WHERE EXTRACT(YEAR FROM pickup_datetime) >= 2015\n",
    "    \"\"\"\n",
    "\n",
    "    # Query f√ºr das Ziel (Canonical + Error)\n",
    "    query_target = f\"\"\"\n",
    "    SELECT source_system as src, 'VALID' as cat, COUNT(*) as anzahl FROM `{table_ref}` GROUP BY 1\n",
    "    UNION ALL\n",
    "    SELECT source_system, 'ERROR', COUNT(*) FROM `{error_table_ref}` GROUP BY 1\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        df_staging = client.query(query_staging).to_dataframe()\n",
    "        df_target = client.query(query_target).to_dataframe()\n",
    "\n",
    "        print(\"\\n--- 1. ROHDATEN IN STAGING (gefiltert) ---\")\n",
    "        print(df_staging)\n",
    "        total_raw = df_staging['anzahl'].sum()\n",
    "        print(f\"Gesamt Rohdaten: {total_raw:,}\")\n",
    "\n",
    "        print(\"\\n--- 2. VERTEILUNG IM CANONICAL LAYER ---\")\n",
    "        print(df_target)\n",
    "        total_target = df_target['anzahl'].sum()\n",
    "        print(f\"Gesamt (Valid + Error): {total_target:,}\")\n",
    "\n",
    "        diff = total_raw - total_target\n",
    "        print(\"\\n--- ANALYSE ---\")\n",
    "        print(f\"Differenz: {diff:,} Zeilen\")\n",
    "        if diff > 0:\n",
    "            print(f\"‚ÑπÔ∏è Hinweis: Diese {diff:,} Zeilen wurden als DUPLIKATE entfernt (Qualify-Regel).\")\n",
    "        elif diff == 0:\n",
    "            print(\" Perfekt: Jede Zeile wurde entweder als VALID oder ERROR verarbeitet.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\" Fehler beim Audit: {e}\")\n",
    "\n",
    "run_data_audit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91c6736d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Analyse Ratecode und Payment Type Mapping f√ºr 2010...\n",
      "\n",
      "--- 1. Verteilung RatecodeID (Ziel: 1=Standard, 2=JFK, etc.) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lania/Documents/GitHub/bi_project_task2/.venv/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    RatecodeID    anzahl     Bezeichnung\n",
      "0            1  14367353        Standard\n",
      "1            2    226068             JFK\n",
      "2            4     44731  Nassau/Westch.\n",
      "3            5     25431            Neg.\n",
      "4            3     17938          Newark\n",
      "5            0     16019             NaN\n",
      "6            6       257           Group\n",
      "7          210        13             NaN\n",
      "8           33         6             NaN\n",
      "9          128         2             NaN\n",
      "10          65         1             NaN\n",
      "11           7         1             NaN\n",
      "\n",
      "--- 2. Verteilung Payment Type (Ziel: 1=Card, 2=Cash, etc.) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lania/Documents/GitHub/bi_project_task2/.venv/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   payment_type   anzahl Bezeichnung\n",
      "0             2  9305785        Cash\n",
      "1             1  5354395        Card\n",
      "2             5    37640     Unknown\n"
     ]
    }
   ],
   "source": [
    "# Zelle 5: Mapping-Check f√ºr Ratecode und Payment Type (Fokus 2010)\n",
    "def check_rate_and_payment():\n",
    "    print(\" Analyse Ratecode und Payment Type Mapping f√ºr 2010...\")\n",
    "\n",
    "    # Query f√ºr Ratecode Verteilung\n",
    "    query_ratecode = f\"\"\"\n",
    "    SELECT \n",
    "        RatecodeID, \n",
    "        COUNT(*) as anzahl\n",
    "    FROM `{table_ref}`\n",
    "    WHERE EXTRACT(YEAR FROM pickup_datetime) = 2010\n",
    "    GROUP BY 1\n",
    "    ORDER BY anzahl DESC\n",
    "    \"\"\"\n",
    "\n",
    "    # Query f√ºr Payment Type Verteilung\n",
    "    query_payment = f\"\"\"\n",
    "    SELECT \n",
    "        payment_type, \n",
    "        COUNT(*) as anzahl\n",
    "    FROM `{table_ref}`\n",
    "    WHERE EXTRACT(YEAR FROM pickup_datetime) = 2010\n",
    "    GROUP BY 1\n",
    "    ORDER BY anzahl DESC\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        print(\"\\n--- 1. Verteilung RatecodeID (Ziel: 1=Standard, 2=JFK, etc.) ---\")\n",
    "        df_rate = client.query(query_ratecode).to_dataframe()\n",
    "        # Mapping Label f√ºr die Anzeige\n",
    "        rate_map = {1: \"Standard\", 2: \"JFK\", 3: \"Newark\", 4: \"Nassau/Westch.\", 5: \"Neg.\", 6: \"Group\", 99: \"Unknown\"}\n",
    "        df_rate['Bezeichnung'] = df_rate['RatecodeID'].map(rate_map)\n",
    "        print(df_rate)\n",
    "\n",
    "        print(\"\\n--- 2. Verteilung Payment Type (Ziel: 1=Card, 2=Cash, etc.) ---\")\n",
    "        df_pay = client.query(query_payment).to_dataframe()\n",
    "        # Mapping Label f√ºr die Anzeige\n",
    "        pay_map = {1: \"Card\", 2: \"Cash\", 3: \"No Charge\", 4: \"Dispute\", 5: \"Unknown\", 6: \"Void\"}\n",
    "        df_pay['Bezeichnung'] = df_pay['payment_type'].map(pay_map)\n",
    "        print(df_pay)\n",
    "\n",
    "        # Erfolgskontrolle\n",
    "        if not df_rate.empty and all(isinstance(x, (int, float)) for x in df_rate['RatecodeID'].dropna()):\n",
    "            print(\"\\n‚úÖ SUCCESS: RatecodeID wurde erfolgreich in numerische Werte transformiert.\")\n",
    "        if not df_pay.empty and all(isinstance(x, (int, float)) for x in df_pay['payment_type'].dropna()):\n",
    "            print(\"‚úÖ SUCCESS: payment_type wurde erfolgreich in numerische Werte transformiert.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei der Analyse: {e}\")\n",
    "\n",
    "check_rate_and_payment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7894d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Analyse der Payment Types (Rohdaten) ---\n",
      "               jahr raw_value    anzahl\n",
      "0   2010 (Schema 5)       Cre   5168999\n",
      "1   2010 (Schema 5)       CAS   5164878\n",
      "2   2010 (Schema 5)       Cas   4227711\n",
      "3   2010 (Schema 5)       CRE    225832\n",
      "4   2010 (Schema 5)       No      31091\n",
      "5   2010 (Schema 5)       Dis      6617\n",
      "6     2023 (Modern)         1  32449126\n",
      "7     2023 (Modern)         2   6957625\n",
      "8     2023 (Modern)         0   1409243\n",
      "9     2023 (Modern)         4    539200\n",
      "10    2023 (Modern)         3    262154\n",
      "11    2023 (Modern)         5         3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lania/Documents/GitHub/bi_project_task2/.venv/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Wir pr√ºfen die Payment Types in den alten 2010er Daten und den neuen 2023er Daten\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    '2010 (Schema 5)' as jahr,\n",
    "    CAST(payment_type AS STRING) as raw_value, \n",
    "    COUNT(*) as anzahl\n",
    "FROM `taxi-bi-project.staging.yellow_schema_5`\n",
    "GROUP BY 1, 2\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    '2023 (Modern)' as jahr,\n",
    "    CAST(payment_type AS STRING) as raw_value, \n",
    "    COUNT(*) as anzahl\n",
    "FROM `taxi-bi-project.staging.yellow_staging_unified`\n",
    "WHERE EXTRACT(YEAR FROM tpep_pickup_datetime) = 2023\n",
    "GROUP BY 1, 2\n",
    "ORDER BY jahr, anzahl DESC\n",
    "\"\"\"\n",
    "\n",
    "df_payments = client.query(query).to_dataframe()\n",
    "print(\"--- Analyse der Payment Types (Rohdaten) ---\")\n",
    "print(df_payments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e12538c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üîç DATA INSPECTION (Tabelle: taxi-bi-project.canonical.canonical_unified_taxi) ---\n",
      "\n",
      "üöï YELLOW TAXI SAMPLE (Sollte Finanzen & Ratecode haben):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lania/Documents/GitHub/bi_project_task2/.venv/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>source_system</th>\n",
       "      <th>load_date</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>Affiliated_base_number</th>\n",
       "      <th>dispatching_base_nummer</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>pickup_location_id</th>\n",
       "      <th>dropoff_location_id</th>\n",
       "      <th>...</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>Airport_fee</th>\n",
       "      <th>ehail_fee</th>\n",
       "      <th>dq_issue_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-658072257274401130</td>\n",
       "      <td>YELLOW</td>\n",
       "      <td>2026-01-12 13:57:51.139251+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-06-27 23:55:23+00:00</td>\n",
       "      <td>2018-06-28 00:43:25+00:00</td>\n",
       "      <td>264</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.31</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8348393098917601581</td>\n",
       "      <td>YELLOW</td>\n",
       "      <td>2026-01-12 13:57:51.139251+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-06-19 00:08:26+00:00</td>\n",
       "      <td>2018-06-19 00:08:28+00:00</td>\n",
       "      <td>142</td>\n",
       "      <td>142</td>\n",
       "      <td>...</td>\n",
       "      <td>30.31</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1460911611762459309</td>\n",
       "      <td>YELLOW</td>\n",
       "      <td>2026-01-12 13:57:51.139251+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-06-01 21:07:22+00:00</td>\n",
       "      <td>2018-06-01 21:07:27+00:00</td>\n",
       "      <td>193</td>\n",
       "      <td>193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3092063686102249450</td>\n",
       "      <td>YELLOW</td>\n",
       "      <td>2026-01-12 13:57:51.139251+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-06-14 22:22:39+00:00</td>\n",
       "      <td>2018-06-14 22:22:47+00:00</td>\n",
       "      <td>193</td>\n",
       "      <td>193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8307824290261541912</td>\n",
       "      <td>YELLOW</td>\n",
       "      <td>2026-01-12 13:57:51.139251+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-06-29 16:13:29+00:00</td>\n",
       "      <td>2018-06-29 16:13:33+00:00</td>\n",
       "      <td>193</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-4166521985088562404</td>\n",
       "      <td>YELLOW</td>\n",
       "      <td>2026-01-12 13:57:51.139251+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-06-18 19:54:11+00:00</td>\n",
       "      <td>2018-06-18 21:29:38+00:00</td>\n",
       "      <td>48</td>\n",
       "      <td>193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-3779308610498269794</td>\n",
       "      <td>YELLOW</td>\n",
       "      <td>2026-01-12 13:57:51.139251+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-06-29 01:03:40+00:00</td>\n",
       "      <td>2018-06-29 01:03:43+00:00</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>1.38</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5690374322425060622</td>\n",
       "      <td>YELLOW</td>\n",
       "      <td>2026-01-12 13:57:51.139251+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-06-08 17:25:06+00:00</td>\n",
       "      <td>2018-06-08 17:26:06+00:00</td>\n",
       "      <td>264</td>\n",
       "      <td>264</td>\n",
       "      <td>...</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3624763093319844491</td>\n",
       "      <td>YELLOW</td>\n",
       "      <td>2026-01-12 13:57:51.139251+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-06-04 15:16:04+00:00</td>\n",
       "      <td>2018-06-04 15:16:59+00:00</td>\n",
       "      <td>264</td>\n",
       "      <td>264</td>\n",
       "      <td>...</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7544656005096956621</td>\n",
       "      <td>YELLOW</td>\n",
       "      <td>2026-01-12 13:57:51.139251+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-06-20 11:41:16+00:00</td>\n",
       "      <td>2018-06-20 11:42:10+00:00</td>\n",
       "      <td>264</td>\n",
       "      <td>264</td>\n",
       "      <td>...</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1028217530261020437</td>\n",
       "      <td>YELLOW</td>\n",
       "      <td>2026-01-12 13:57:51.139251+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-06-14 16:44:23+00:00</td>\n",
       "      <td>2018-06-14 16:45:22+00:00</td>\n",
       "      <td>264</td>\n",
       "      <td>264</td>\n",
       "      <td>...</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6714242136683301518</td>\n",
       "      <td>YELLOW</td>\n",
       "      <td>2026-01-12 13:57:51.139251+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-06-06 11:44:26+00:00</td>\n",
       "      <td>2018-06-06 11:44:41+00:00</td>\n",
       "      <td>264</td>\n",
       "      <td>264</td>\n",
       "      <td>...</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3679386380600090248</td>\n",
       "      <td>YELLOW</td>\n",
       "      <td>2026-01-12 13:57:51.139251+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-06-08 11:46:50+00:00</td>\n",
       "      <td>2018-06-08 11:48:04+00:00</td>\n",
       "      <td>264</td>\n",
       "      <td>264</td>\n",
       "      <td>...</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-2640647145534205410</td>\n",
       "      <td>YELLOW</td>\n",
       "      <td>2026-01-12 13:57:51.139251+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-06-04 15:35:16+00:00</td>\n",
       "      <td>2018-06-04 15:36:43+00:00</td>\n",
       "      <td>264</td>\n",
       "      <td>264</td>\n",
       "      <td>...</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-2187302081072143541</td>\n",
       "      <td>YELLOW</td>\n",
       "      <td>2026-01-12 13:57:51.139251+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-06-23 15:43:30+00:00</td>\n",
       "      <td>2018-06-23 15:44:56+00:00</td>\n",
       "      <td>264</td>\n",
       "      <td>264</td>\n",
       "      <td>...</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows √ó 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 trip_id source_system                        load_date  \\\n",
       "0    -658072257274401130        YELLOW 2026-01-12 13:57:51.139251+00:00   \n",
       "1    8348393098917601581        YELLOW 2026-01-12 13:57:51.139251+00:00   \n",
       "2    1460911611762459309        YELLOW 2026-01-12 13:57:51.139251+00:00   \n",
       "3    3092063686102249450        YELLOW 2026-01-12 13:57:51.139251+00:00   \n",
       "4    8307824290261541912        YELLOW 2026-01-12 13:57:51.139251+00:00   \n",
       "5   -4166521985088562404        YELLOW 2026-01-12 13:57:51.139251+00:00   \n",
       "6   -3779308610498269794        YELLOW 2026-01-12 13:57:51.139251+00:00   \n",
       "7    5690374322425060622        YELLOW 2026-01-12 13:57:51.139251+00:00   \n",
       "8    3624763093319844491        YELLOW 2026-01-12 13:57:51.139251+00:00   \n",
       "9    7544656005096956621        YELLOW 2026-01-12 13:57:51.139251+00:00   \n",
       "10  -1028217530261020437        YELLOW 2026-01-12 13:57:51.139251+00:00   \n",
       "11   6714242136683301518        YELLOW 2026-01-12 13:57:51.139251+00:00   \n",
       "12   3679386380600090248        YELLOW 2026-01-12 13:57:51.139251+00:00   \n",
       "13  -2640647145534205410        YELLOW 2026-01-12 13:57:51.139251+00:00   \n",
       "14  -2187302081072143541        YELLOW 2026-01-12 13:57:51.139251+00:00   \n",
       "\n",
       "   vendor_id Affiliated_base_number dispatching_base_nummer  \\\n",
       "0          2                   None                    None   \n",
       "1          2                   None                    None   \n",
       "2          2                   None                    None   \n",
       "3          2                   None                    None   \n",
       "4          2                   None                    None   \n",
       "5          2                   None                    None   \n",
       "6          2                   None                    None   \n",
       "7          2                   None                    None   \n",
       "8          2                   None                    None   \n",
       "9          2                   None                    None   \n",
       "10         2                   None                    None   \n",
       "11         2                   None                    None   \n",
       "12         2                   None                    None   \n",
       "13         2                   None                    None   \n",
       "14         2                   None                    None   \n",
       "\n",
       "             pickup_datetime          dropoff_datetime  pickup_location_id  \\\n",
       "0  2018-06-27 23:55:23+00:00 2018-06-28 00:43:25+00:00                 264   \n",
       "1  2018-06-19 00:08:26+00:00 2018-06-19 00:08:28+00:00                 142   \n",
       "2  2018-06-01 21:07:22+00:00 2018-06-01 21:07:27+00:00                 193   \n",
       "3  2018-06-14 22:22:39+00:00 2018-06-14 22:22:47+00:00                 193   \n",
       "4  2018-06-29 16:13:29+00:00 2018-06-29 16:13:33+00:00                 193   \n",
       "5  2018-06-18 19:54:11+00:00 2018-06-18 21:29:38+00:00                  48   \n",
       "6  2018-06-29 01:03:40+00:00 2018-06-29 01:03:43+00:00                  36   \n",
       "7  2018-06-08 17:25:06+00:00 2018-06-08 17:26:06+00:00                 264   \n",
       "8  2018-06-04 15:16:04+00:00 2018-06-04 15:16:59+00:00                 264   \n",
       "9  2018-06-20 11:41:16+00:00 2018-06-20 11:42:10+00:00                 264   \n",
       "10 2018-06-14 16:44:23+00:00 2018-06-14 16:45:22+00:00                 264   \n",
       "11 2018-06-06 11:44:26+00:00 2018-06-06 11:44:41+00:00                 264   \n",
       "12 2018-06-08 11:46:50+00:00 2018-06-08 11:48:04+00:00                 264   \n",
       "13 2018-06-04 15:35:16+00:00 2018-06-04 15:36:43+00:00                 264   \n",
       "14 2018-06-23 15:43:30+00:00 2018-06-23 15:44:56+00:00                 264   \n",
       "\n",
       "    dropoff_location_id  ...  total_amount  payment_type extra  mta_tax  \\\n",
       "0                   100  ...          0.31             2   0.0      0.0   \n",
       "1                   142  ...         30.31             1   0.0      0.0   \n",
       "2                   193  ...          0.35             1   0.0      0.0   \n",
       "3                   193  ...          0.35             1   0.0      0.0   \n",
       "4                     7  ...          0.35             1   0.0      0.0   \n",
       "5                   193  ...          0.81             1   0.0      0.0   \n",
       "6                    36  ...          1.38             1   0.0      0.5   \n",
       "7                   264  ...          1.80             2   0.0      0.5   \n",
       "8                   264  ...          1.80             2   0.0      0.5   \n",
       "9                   264  ...          1.80             2   0.0      0.5   \n",
       "10                  264  ...          1.80             2   0.0      0.5   \n",
       "11                  264  ...          1.80             2   0.0      0.5   \n",
       "12                  264  ...          1.90             2   0.0      0.5   \n",
       "13                  264  ...          1.90             2   0.0      0.5   \n",
       "14                  264  ...          1.90             2   0.0      0.5   \n",
       "\n",
       "    tolls_amount  improvement_surcharge  congestion_surcharge  Airport_fee  \\\n",
       "0            0.0                    0.3                   0.0          0.0   \n",
       "1            0.0                    0.3                   0.0          0.0   \n",
       "2            0.0                    0.3                   0.0          0.0   \n",
       "3            0.0                    0.3                   0.0          0.0   \n",
       "4            0.0                    0.3                   0.0          0.0   \n",
       "5            0.0                    0.3                   0.0          0.0   \n",
       "6            0.0                    0.3                   0.0          0.0   \n",
       "7            0.0                    0.3                   0.0          0.0   \n",
       "8            0.0                    0.3                   0.0          0.0   \n",
       "9            0.0                    0.3                   0.0          0.0   \n",
       "10           0.0                    0.3                   0.0          0.0   \n",
       "11           0.0                    0.3                   0.0          0.0   \n",
       "12           0.0                    0.3                   0.0          0.0   \n",
       "13           0.0                    0.3                   0.0          0.0   \n",
       "14           0.0                    0.3                   0.0          0.0   \n",
       "\n",
       "    ehail_fee  dq_issue_flag  \n",
       "0         NaN           True  \n",
       "1         NaN          False  \n",
       "2         NaN          False  \n",
       "3         NaN          False  \n",
       "4         NaN          False  \n",
       "5         NaN          False  \n",
       "6         NaN          False  \n",
       "7         NaN           True  \n",
       "8         NaN           True  \n",
       "9         NaN           True  \n",
       "10        NaN           True  \n",
       "11        NaN           True  \n",
       "12        NaN           True  \n",
       "13        NaN           True  \n",
       "14        NaN           True  \n",
       "\n",
       "[15 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíö GREEN TAXI SAMPLE (Sollte Trip_type & ehail_fee haben):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lania/Documents/GitHub/bi_project_task2/.venv/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>source_system</th>\n",
       "      <th>load_date</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>Affiliated_base_number</th>\n",
       "      <th>dispatching_base_nummer</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>pickup_location_id</th>\n",
       "      <th>dropoff_location_id</th>\n",
       "      <th>...</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>Airport_fee</th>\n",
       "      <th>ehail_fee</th>\n",
       "      <th>dq_issue_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6123691602657471060</td>\n",
       "      <td>GREEN</td>\n",
       "      <td>2026-01-12 13:57:51.139251+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-01-08 20:03:40+00:00</td>\n",
       "      <td>2018-01-08 20:13:34+00:00</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.31</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-6891589405852261993</td>\n",
       "      <td>GREEN</td>\n",
       "      <td>2026-01-12 13:57:51.139251+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-01-16 08:47:12+00:00</td>\n",
       "      <td>2018-01-16 09:29:48+00:00</td>\n",
       "      <td>198</td>\n",
       "      <td>164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.31</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4693742452531165115</td>\n",
       "      <td>GREEN</td>\n",
       "      <td>2026-01-12 13:57:51.139251+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-01-26 10:02:05+00:00</td>\n",
       "      <td>2018-01-26 10:04:59+00:00</td>\n",
       "      <td>33</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>0.31</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990537406338895965</td>\n",
       "      <td>GREEN</td>\n",
       "      <td>2026-01-12 13:57:51.139251+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-01-18 17:41:09+00:00</td>\n",
       "      <td>2018-01-18 17:54:57+00:00</td>\n",
       "      <td>160</td>\n",
       "      <td>82</td>\n",
       "      <td>...</td>\n",
       "      <td>0.31</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>993981608076748676</td>\n",
       "      <td>GREEN</td>\n",
       "      <td>2026-01-12 13:57:51.139251+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-01-18 11:39:51+00:00</td>\n",
       "      <td>2018-01-18 11:51:20+00:00</td>\n",
       "      <td>160</td>\n",
       "      <td>226</td>\n",
       "      <td>...</td>\n",
       "      <td>0.31</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                trip_id source_system                        load_date  \\\n",
       "0   6123691602657471060         GREEN 2026-01-12 13:57:51.139251+00:00   \n",
       "1  -6891589405852261993         GREEN 2026-01-12 13:57:51.139251+00:00   \n",
       "2  -4693742452531165115         GREEN 2026-01-12 13:57:51.139251+00:00   \n",
       "3   1990537406338895965         GREEN 2026-01-12 13:57:51.139251+00:00   \n",
       "4    993981608076748676         GREEN 2026-01-12 13:57:51.139251+00:00   \n",
       "\n",
       "  vendor_id Affiliated_base_number dispatching_base_nummer  \\\n",
       "0         1                   None                    None   \n",
       "1         1                   None                    None   \n",
       "2         1                   None                    None   \n",
       "3         1                   None                    None   \n",
       "4         1                   None                    None   \n",
       "\n",
       "            pickup_datetime          dropoff_datetime  pickup_location_id  \\\n",
       "0 2018-01-08 20:03:40+00:00 2018-01-08 20:13:34+00:00                 198   \n",
       "1 2018-01-16 08:47:12+00:00 2018-01-16 09:29:48+00:00                 198   \n",
       "2 2018-01-26 10:02:05+00:00 2018-01-26 10:04:59+00:00                  33   \n",
       "3 2018-01-18 17:41:09+00:00 2018-01-18 17:54:57+00:00                 160   \n",
       "4 2018-01-18 11:39:51+00:00 2018-01-18 11:51:20+00:00                 160   \n",
       "\n",
       "   dropoff_location_id  ...  total_amount  payment_type extra  mta_tax  \\\n",
       "0                  198  ...          0.31             3   0.0      0.0   \n",
       "1                  164  ...          0.31             2   0.0      0.0   \n",
       "2                   65  ...          0.31             3   0.0      0.0   \n",
       "3                   82  ...          0.31             2   0.0      0.0   \n",
       "4                  226  ...          0.31             2   0.0      0.0   \n",
       "\n",
       "   tolls_amount  improvement_surcharge  congestion_surcharge  Airport_fee  \\\n",
       "0           0.0                    0.3                   NaN          0.0   \n",
       "1           0.0                    0.3                   NaN          0.0   \n",
       "2           0.0                    0.3                   NaN          0.0   \n",
       "3           0.0                    0.3                   NaN          0.0   \n",
       "4           0.0                    0.3                   NaN          0.0   \n",
       "\n",
       "   ehail_fee  dq_issue_flag  \n",
       "0        NaN          False  \n",
       "1        NaN           True  \n",
       "2        NaN          False  \n",
       "3        NaN           True  \n",
       "4        NaN           True  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üñ§ FHV SAMPLE (Muss Base-Nummern haben, aber KEINE Preise):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lania/Documents/GitHub/bi_project_task2/.venv/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>source_system</th>\n",
       "      <th>load_date</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>Affiliated_base_number</th>\n",
       "      <th>dispatching_base_nummer</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>pickup_location_id</th>\n",
       "      <th>dropoff_location_id</th>\n",
       "      <th>...</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>Airport_fee</th>\n",
       "      <th>ehail_fee</th>\n",
       "      <th>dq_issue_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1456451390256010396</td>\n",
       "      <td>FHV</td>\n",
       "      <td>2026-01-12 13:57:51.139251+00:00</td>\n",
       "      <td>99</td>\n",
       "      <td>B01727</td>\n",
       "      <td>B01727</td>\n",
       "      <td>2016-11-15 18:15:00+00:00</td>\n",
       "      <td>2018-11-15 23:26:00+00:00</td>\n",
       "      <td>264</td>\n",
       "      <td>264</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2962142395979994425</td>\n",
       "      <td>FHV</td>\n",
       "      <td>2026-01-12 13:57:51.139251+00:00</td>\n",
       "      <td>99</td>\n",
       "      <td>B02207</td>\n",
       "      <td>B00823</td>\n",
       "      <td>2017-07-25 14:30:00+00:00</td>\n",
       "      <td>2017-07-25 15:06:10+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>257</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>526033488035429905</td>\n",
       "      <td>FHV</td>\n",
       "      <td>2026-01-12 13:57:51.139251+00:00</td>\n",
       "      <td>99</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>B01268</td>\n",
       "      <td>2017-07-22 19:00:48+00:00</td>\n",
       "      <td>2017-07-22 20:00:51+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>221</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4763764091378471155</td>\n",
       "      <td>FHV</td>\n",
       "      <td>2026-01-12 13:57:51.139251+00:00</td>\n",
       "      <td>99</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>B01268</td>\n",
       "      <td>2017-07-10 08:00:48+00:00</td>\n",
       "      <td>2017-07-10 09:00:19+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>206</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-8721102308068264949</td>\n",
       "      <td>FHV</td>\n",
       "      <td>2026-01-12 13:57:51.139251+00:00</td>\n",
       "      <td>99</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>B01268</td>\n",
       "      <td>2017-07-11 21:00:25+00:00</td>\n",
       "      <td>2017-07-12 00:00:06+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>251</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                trip_id source_system                        load_date  \\\n",
       "0  -1456451390256010396           FHV 2026-01-12 13:57:51.139251+00:00   \n",
       "1  -2962142395979994425           FHV 2026-01-12 13:57:51.139251+00:00   \n",
       "2    526033488035429905           FHV 2026-01-12 13:57:51.139251+00:00   \n",
       "3  -4763764091378471155           FHV 2026-01-12 13:57:51.139251+00:00   \n",
       "4  -8721102308068264949           FHV 2026-01-12 13:57:51.139251+00:00   \n",
       "\n",
       "  vendor_id Affiliated_base_number dispatching_base_nummer  \\\n",
       "0        99                 B01727                  B01727   \n",
       "1        99                 B02207                  B00823   \n",
       "2        99                UNKNOWN                  B01268   \n",
       "3        99                UNKNOWN                  B01268   \n",
       "4        99                UNKNOWN                  B01268   \n",
       "\n",
       "            pickup_datetime          dropoff_datetime  pickup_location_id  \\\n",
       "0 2016-11-15 18:15:00+00:00 2018-11-15 23:26:00+00:00                 264   \n",
       "1 2017-07-25 14:30:00+00:00 2017-07-25 15:06:10+00:00                   1   \n",
       "2 2017-07-22 19:00:48+00:00 2017-07-22 20:00:51+00:00                   1   \n",
       "3 2017-07-10 08:00:48+00:00 2017-07-10 09:00:19+00:00                   1   \n",
       "4 2017-07-11 21:00:25+00:00 2017-07-12 00:00:06+00:00                   1   \n",
       "\n",
       "   dropoff_location_id  ...  total_amount  payment_type extra  mta_tax  \\\n",
       "0                  264  ...           NaN             0   NaN      NaN   \n",
       "1                  257  ...           NaN             0   NaN      NaN   \n",
       "2                  221  ...           NaN             0   NaN      NaN   \n",
       "3                  206  ...           NaN             0   NaN      NaN   \n",
       "4                  251  ...           NaN             0   NaN      NaN   \n",
       "\n",
       "   tolls_amount  improvement_surcharge  congestion_surcharge  Airport_fee  \\\n",
       "0           NaN                    NaN                   NaN          NaN   \n",
       "1           NaN                    NaN                   NaN          NaN   \n",
       "2           NaN                    NaN                   NaN          NaN   \n",
       "3           NaN                    NaN                   NaN          NaN   \n",
       "4           NaN                    NaN                   NaN          NaN   \n",
       "\n",
       "   ehail_fee  dq_issue_flag  \n",
       "0        NaN          False  \n",
       "1        NaN          False  \n",
       "2        NaN          False  \n",
       "3        NaN          False  \n",
       "4        NaN          False  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä VERTEILUNG NACH SYSTEM:\n",
      "  source_system      count\n",
      "0           FHV  510080635\n",
      "1        YELLOW  172064797\n",
      "2         GREEN   67447828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lania/Documents/GitHub/bi_project_task2/.venv/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Zelle 5: Data Quality Check (Stichproben pro System)\n",
    "import pandas as pd\n",
    "\n",
    "def check_data_samples():\n",
    "    print(f\"--- üîç DATA INSPECTION (Tabelle: {table_ref}) ---\\n\")\n",
    "    \n",
    "    # 1. YELLOW CHECK (\"Cellos\")\n",
    "    # Fokus: Haben sie Ratecode? Sind Finanzen da?\n",
    "    print(\"üöï YELLOW TAXI SAMPLE (Sollte Finanzen & Ratecode haben):\")\n",
    "    sql_yellow = f\"\"\"\n",
    "    SELECT \n",
    "        *\n",
    "    FROM `{table_ref}`\n",
    "    WHERE source_system = 'YELLOW'\n",
    "    LIMIT 15\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_y = client.query(sql_yellow).to_dataframe()\n",
    "        display(df_y) # Oder print(df_y) falls kein Jupyter\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # 2. GREEN CHECK\n",
    "    # Fokus: Haben sie Trip_type und Ehail_fee? (Spezifisch f√ºr Green)\n",
    "    print(\"\\nüíö GREEN TAXI SAMPLE (Sollte Trip_type & ehail_fee haben):\")\n",
    "    sql_green = f\"\"\"\n",
    "    SELECT \n",
    "        *\n",
    "    FROM `{table_ref}`\n",
    "    WHERE source_system = 'GREEN'\n",
    "    LIMIT 5\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_g = client.query(sql_green).to_dataframe()\n",
    "        display(df_g)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # 3. FHV CHECK\n",
    "    # Fokus: Sind die neuen Base-Nummern da? Sind Preise WIRKLICH NULL?\n",
    "    print(\"\\nüñ§ FHV SAMPLE (Muss Base-Nummern haben, aber KEINE Preise):\")\n",
    "    sql_fhv = f\"\"\"\n",
    "    SELECT \n",
    "        *\n",
    "    FROM `{table_ref}`\n",
    "    WHERE source_system = 'FHV'\n",
    "    LIMIT 5\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_f = client.query(sql_fhv).to_dataframe()\n",
    "        display(df_f)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # 4. STATISTIK\n",
    "    print(\"\\nüìä VERTEILUNG NACH SYSTEM:\")\n",
    "    sql_stats = f\"\"\"\n",
    "    SELECT source_system, COUNT(*) as count \n",
    "    FROM `{table_ref}` \n",
    "    GROUP BY source_system\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(client.query(sql_stats).to_dataframe())\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "check_data_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2cd6f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/88/9clxv4m57vjb1lzsxk3gkv2m0000gn/T/ipykernel_50106/1406648598.py:16: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
      "  df_to_upload.to_gbq('staging.taxi_zones_temp', project_id='taxi-bi-project', if_exists='replace')\n",
      "263 out of 263 rows loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tabelle taxi_zones_geo wurde mit 'borough' und 'zone' neu erstellt!\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "# 1. GeoJSON mit allen Spalten laden\n",
    "geojson_path = '/Users/lania/Documents/GitHub/bi_project_task2/notebook_canonical/NYC_Taxi_Zones.geojson'\n",
    "gdf_zones = gpd.read_file(geojson_path)\n",
    "\n",
    "# 2. Alle wichtigen Spalten vorbereiten\n",
    "# Wir brauchen: location_id, zone, borough und die Geometrie als WKT\n",
    "gdf_zones['WKT_GEOMETRY'] = gdf_zones['geometry'].apply(lambda x: x.wkt)\n",
    "df_to_upload = gdf_zones[['location_id', 'zone', 'borough', 'WKT_GEOMETRY']].copy()\n",
    "\n",
    "# 3. Hochladen nach BigQuery (Temp-Tabelle)\n",
    "df_to_upload.to_gbq('staging.taxi_zones_temp', project_id='taxi-bi-project', if_exists='replace')\n",
    "\n",
    "# 4. Die finale Geo-Tabelle mit ALLEN Spalten erstellen\n",
    "fix_query = \"\"\"\n",
    "CREATE OR REPLACE TABLE `taxi-bi-project.staging.taxi_zones_geo` AS\n",
    "SELECT \n",
    "    CAST(location_id AS INT64) AS location_id,\n",
    "    zone,\n",
    "    borough,\n",
    "    ST_GEOGFROMTEXT(WKT_GEOMETRY) AS zone_geom\n",
    "FROM `taxi-bi-project.staging.taxi_zones_temp`;\n",
    "\"\"\"\n",
    "client.query(fix_query, location=\"EU\").result()\n",
    "\n",
    "print(\"‚úÖ Tabelle taxi_zones_geo wurde mit 'borough' und 'zone' neu erstellt!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ee505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abfrage der Original-VendorIDs aus yellow_schema_5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lania/Documents/GitHub/bi_project_task2/.venv/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Gefundene Anbieter in Schema 5 (2010) ---\n",
      "  vendor_id   anzahl\n",
      "0       VTS  7471719\n",
      "1       CMT  6670024\n",
      "2       DDS   683385\n"
     ]
    }
   ],
   "source": [
    "#Check Vendors in Yellow \n",
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Wir fragen direkt die Tabelle yellow_schema_5 ab\n",
    "# Ich nutze hier 'vendor_id', da dies der Standardname in Schema 5 ist.\n",
    "# Falls es wirklich 'vendor_id' geschrieben wird, passen wir es an.\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    vendor_id, \n",
    "    COUNT(*) as anzahl\n",
    "FROM `taxi-bi-project.staging.yellow_schema_5`\n",
    "GROUP BY 1\n",
    "ORDER BY anzahl DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"Abfrage der Original-VendorIDs aus yellow_schema_5...\")\n",
    "\n",
    "try:\n",
    "    df_vendor_5 = client.query(query, location=\"EU\").to_dataframe()\n",
    "    print(\"\\n--- Gefundene Anbieter in Schema 5 (2010) ---\")\n",
    "    print(df_vendor_5)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Fehler: {e}\")\n",
    "    print(\"\\nHinweis: Falls die Spalte anders geschrieben wird (z.B. vendor_name), versuche die Spaltenliste anzuzeigen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "355c95d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   source_system  year                         rejection_reason       cnt\n",
      "0            FHV  2016              Incorrect: Invalid Duration  93213193\n",
      "1            FHV  2017              Incorrect: Invalid Duration  56434432\n",
      "2            FHV  2015              Incorrect: Invalid Duration  41629912\n",
      "3            FHV  2016                         DUPLICATE_RECORD  38899810\n",
      "4            FHV  2015                         DUPLICATE_RECORD  21757851\n",
      "5            FHV  2017                         DUPLICATE_RECORD  16856399\n",
      "6         YELLOW  2023                         DUPLICATE_RECORD   3641139\n",
      "7            FHV  2024                         DUPLICATE_RECORD    724690\n",
      "8            FHV  2019                         DUPLICATE_RECORD    704331\n",
      "9            FHV  2021                         DUPLICATE_RECORD    614120\n",
      "10           FHV  2018                         DUPLICATE_RECORD    609335\n",
      "11           FHV  2022                         DUPLICATE_RECORD    603533\n",
      "12           FHV  2020                         DUPLICATE_RECORD    582183\n",
      "13        YELLOW  2023  Incorrect: Invalid Pax Count (Rule 1.4)    581619\n",
      "14           FHV  2025                         DUPLICATE_RECORD    539617\n",
      "15           FHV  2023                         DUPLICATE_RECORD    430720\n",
      "16        YELLOW  2025                    Incorrect: Financials    244125\n",
      "17        YELLOW  2011  Incorrect: Invalid Pax Count (Rule 1.4)    238271\n",
      "18        YELLOW  2023                    Incorrect: Financials    225614\n",
      "19        YELLOW  2019  Incorrect: Invalid Pax Count (Rule 1.4)    127826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lania/Documents/GitHub/bi_project_task2/.venv/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Debug Query: Woher kommen die 402 Millionen Fehler?\n",
    "query_debug = f\"\"\"\n",
    "SELECT \n",
    "    source_system, \n",
    "    EXTRACT(YEAR FROM pickup_datetime) as year,\n",
    "    rejection_reason, \n",
    "    COUNT(*) as cnt\n",
    "FROM `{error_table_ref}`\n",
    "GROUP BY 1, 2, 3\n",
    "ORDER BY cnt DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "df_debug = client.query(query_debug).to_dataframe()\n",
    "print(df_debug)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d77b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sorgt daf√ºr, dass Plots im Notebook angezeigt werden\n",
    "%matplotlib inline\n",
    "\n",
    "# Lesbarkeit in der Exploration erh√∂hen\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "pd.set_option(\"display.width\", 140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c93bf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ QUELLE GEFUNDEN: Dataset 'dimensional' liegt in Region: 'EU'\n"
     ]
    }
   ],
   "source": [
    "# Zelle 1 & 2: Setup mit automatischer Regionen-Korrektur\n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import NotFound\n",
    "import logging\n",
    "import uuid\n",
    "import datetime\n",
    "\n",
    "# Logging Setup\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# --- CONFIG ---\n",
    "PROJECT_ID = \"taxi-bi-project\" # Deine ID aus dem Log\n",
    "DIM_DATASET = \"dimensional\"\n",
    "SOURCE_DATASET = \"staging\"      \n",
    "TARGET_DATASET = \"canonical\"   \n",
    "\n",
    "# Tabellen\n",
    "TARGET_TABLE = \"dimensional\"\n",
    "\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "try:\n",
    "    src_ds_ref = client.get_dataset(f\"{PROJECT_ID}.{DIM_DATASET}\")\n",
    "    CORRECT_LOCATION = src_ds_ref.location\n",
    "    print(f\"‚úÖ QUELLE GEFUNDEN: Dataset '{DIM_DATASET}' liegt in Region: '{CORRECT_LOCATION}'\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå KRITISCHER FEHLER: Konnte Quell-Dataset '{DIM_DATASET}' nicht finden!\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5de23220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Lese GeoJSON 'NYC_Taxi_Zones.geojson' ---\n",
      "Gelesen: 263 Zonen.\n",
      "‚úÖ Tabelle 'taxi-bi-project.dimensional.staging_taxi_zones' erfolgreich hochgeladen.\n"
     ]
    }
   ],
   "source": [
    "# Zelle 4a: GeoJSON Upload (Direkt ins Dimensional Dataset)\n",
    "import json\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# CONFIG\n",
    "GEOJSON_FILE = \"NYC_Taxi_Zones.geojson\" \n",
    "# √ÑNDERUNG: Wir laden es direkt in das 'dimensional' Dataset\n",
    "STAGING_TABLE = f\"{PROJECT_ID}.dimensional.staging_taxi_zones\"\n",
    "\n",
    "def upload_geojson_to_bq():\n",
    "    print(f\"--- Lese GeoJSON '{GEOJSON_FILE}' ---\")\n",
    "    \n",
    "    try:\n",
    "        with open(GEOJSON_FILE, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Datei '{GEOJSON_FILE}' nicht gefunden. Bitte hochladen!\")\n",
    "        return\n",
    "\n",
    "    # Dataset 'dimensional' sicherstellen (falls Zelle 4a vor 4b l√§uft)\n",
    "    try:\n",
    "        client.get_dataset(f\"{PROJECT_ID}.dimensional\")\n",
    "    except:\n",
    "        new_ds = bigquery.Dataset(f\"{PROJECT_ID}.dimensional\")\n",
    "        new_ds.location = CORRECT_LOCATION\n",
    "        client.create_dataset(new_ds)\n",
    "        print(\"‚úÖ Dataset 'dimensional' erstellt.\")\n",
    "\n",
    "    rows = []\n",
    "    for feature in data['features']:\n",
    "        props = feature['properties']\n",
    "        geom = feature['geometry']\n",
    "        \n",
    "        # Flexibles Mapping\n",
    "        loc_id = props.get('LocationID') or props.get('objectid') or props.get('OBJECTID')\n",
    "        \n",
    "        row = {\n",
    "            'LocationID': loc_id,\n",
    "            'Zone': props.get('Zone') or props.get('zone'),\n",
    "            'Borough': props.get('Borough') or props.get('borough'),\n",
    "            'service_zone': props.get('service_zone'),\n",
    "            'geometry_json': json.dumps(geom)\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # FIX: Pandas zwingen, IDs als Zahl zu behandeln\n",
    "    df['LocationID'] = pd.to_numeric(df['LocationID'], errors='coerce').fillna(0).astype(int)\n",
    "    \n",
    "    print(f\"Gelesen: {len(df)} Zonen.\")\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=\"WRITE_TRUNCATE\",\n",
    "        schema=[\n",
    "            bigquery.SchemaField(\"LocationID\", \"INTEGER\"),\n",
    "            bigquery.SchemaField(\"Zone\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"Borough\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"service_zone\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"geometry_json\", \"STRING\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        job = client.load_table_from_dataframe(df, STAGING_TABLE, job_config=job_config)\n",
    "        job.result()\n",
    "        print(f\"‚úÖ Tabelle '{STAGING_TABLE}' erfolgreich hochgeladen.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Upload Fehler: {e}\")\n",
    "\n",
    "upload_geojson_to_bq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33531f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 4: Dimensional Layer (Komplett mit Smart-Base-Logik)\n",
    "def create_dimensional_layer_advanced():\n",
    "    dim_dataset_id = f\"{PROJECT_ID}.{DIM_DATASET}\"\n",
    "    \n",
    "    print(f\"--- Erstelle Advanced Layer 3 in '{DIM_DATASET}' ---\")\n",
    "\n",
    "    # Dataset erstellen\n",
    "    try:\n",
    "        client.get_dataset(dim_dataset_id)\n",
    "    except:\n",
    "        new_ds = bigquery.Dataset(dim_dataset_id)\n",
    "        new_ds.location = CORRECT_LOCATION\n",
    "        client.create_dataset(new_ds)\n",
    "\n",
    "    # 1. DIM: Payment & RateCode & TripType (Die einfachen Labels)\n",
    "    # ... (Code wie oben, hier abgek√ºrzt der √úbersicht halber) ...\n",
    "    client.query(f\"CREATE OR REPLACE TABLE `{dim_dataset_id}.dim_payment_type` AS SELECT * FROM UNNEST([STRUCT(1 as payment_type_id, 'Credit Card' as payment_description), STRUCT(2, 'Cash'), STRUCT(0, 'Unknown')])\", location=CORRECT_LOCATION).result()\n",
    "    client.query(f\"CREATE OR REPLACE TABLE `{dim_dataset_id}.dim_rate_code` AS SELECT * FROM UNNEST([STRUCT(1 as rate_code_id, 'Standard' as rate_description), STRUCT(2, 'JFK'), STRUCT(99, 'Unknown')])\", location=CORRECT_LOCATION).result()\n",
    "    client.query(f\"CREATE OR REPLACE TABLE `{dim_dataset_id}.dim_trip_type` AS SELECT * FROM UNNEST([STRUCT(1 as trip_type_id, 'Street-hail' as trip_description), STRUCT(2, 'Dispatch')])\", location=CORRECT_LOCATION).result()\n",
    "\n",
    "    # 2. DIM: VENDOR (Die intelligente Logik!)\n",
    "    print(\"Erstelle dim_vendor mit Fallback-Logik f√ºr alte Basen...\")\n",
    "    \n",
    "    sql_vendor_smart = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{dim_dataset_id}.dim_vendor`\n",
    "    (vendor_id STRING, vendor_name STRING, source_system STRING, is_active BOOLEAN)\n",
    "    AS\n",
    "    WITH all_existing_bases AS (\n",
    "        -- 1. Wir schauen in die Vergangenheit: Wer ist alles gefahren?\n",
    "        SELECT DISTINCT dispatching_base_nummer as vid FROM `{PROJECT_ID}.{TARGET_DATASET}.canonical_unified_taxi`\n",
    "        WHERE dispatching_base_nummer IS NOT NULL\n",
    "    ),\n",
    "    active_bases_list AS (\n",
    "        -- 2. Deine hochgeladene Datei (Nur die Aktiven)\n",
    "        -- PASS AUF DIE SPALTEN-NAMEN AUF! (Hier: license_number, name)\n",
    "        SELECT base_num, base_name FROM `{PROJECT_ID}.{SOURCE_DATASET}.staging_fhv_bases`\n",
    "    )\n",
    "    \n",
    "    SELECT \n",
    "        hist.vid as vendor_id,\n",
    "        -- Hier ist der \"Smart Logic\" Teil:\n",
    "        COALESCE(act.base_name, CONCAT('Inactive/Unknown Base ', hist.vid)) as vendor_name,\n",
    "        'FHV' as source_system,\n",
    "        IF(act.base_name IS NOT NULL, TRUE, FALSE) as is_active\n",
    "    FROM all_existing_bases hist\n",
    "    LEFT JOIN active_bases_list act ON hist.vid = act.base_num\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    -- Nicht vergessen: Die Yellow/Green Taxis\n",
    "    SELECT '1', 'Creative Mobile Technologies (CMT)', 'YELLOW/GREEN', TRUE\n",
    "    UNION ALL\n",
    "    SELECT '2', 'VeriFone Inc. (Curb)', 'YELLOW/GREEN', TRUE\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        client.query(sql_vendor_smart, location=CORRECT_LOCATION).result()\n",
    "        print(\"‚úÖ dim_vendor erstellt (Alte Basen wurden erhalten).\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Fehler bei dim_vendor (Hast du die Datei hochgeladen?): {e}\")\n",
    "\n",
    "    # 3. DIM: LOCATION (Optional - wenn du die Datei hast)\n",
    "    # Wenn du 'staging_taxi_zones' hochgeladen hast, nimm diesen Block:\n",
    "    \"\"\"\n",
    "    sql_location = f'''\n",
    "    CREATE OR REPLACE TABLE `{dim_dataset_id}.dim_location` AS\n",
    "    SELECT \n",
    "        LocationID, \n",
    "        Borough, \n",
    "        Zone, \n",
    "        service_zone \n",
    "    FROM `{PROJECT_ID}.{SOURCE_DATASET}.staging_taxi_zones`\n",
    "    '''\n",
    "    client.query(sql_location, location=CORRECT_LOCATION).result()\n",
    "    \"\"\"\n",
    "\n",
    "create_dimensional_layer_advanced()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df77383e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Erstelle Layer 3 (Dimensionen) in 'dimensional' ---\n",
      "‚úÖ dim_payment_type erstellt.\n",
      "‚úÖ dim_rate_code erstellt.\n",
      "‚úÖ dim_trip_type erstellt.\n",
      "‚úÖ dim_vendor erstellt (Nur Static Vendors).\n",
      "‚úÖ dim_location erstellt (Maps).\n"
     ]
    }
   ],
   "source": [
    "# Zelle 4: Dimensional Layer (Final Clean Version)\n",
    "def create_dimensional_layer_final():\n",
    "    DIM_DATASET = \"dimensional\"\n",
    "    dim_dataset_id = f\"{PROJECT_ID}.{DIM_DATASET}\"\n",
    "    \n",
    "    print(f\"--- Erstelle Layer 3 (Dimensionen) in '{DIM_DATASET}' ---\")\n",
    "\n",
    "    # 1. Dataset anlegen\n",
    "    try:\n",
    "        client.get_dataset(dim_dataset_id)\n",
    "    except:\n",
    "        new_ds = bigquery.Dataset(dim_dataset_id)\n",
    "        new_ds.location = CORRECT_LOCATION\n",
    "        client.create_dataset(new_ds)\n",
    "        print(f\"‚úÖ Dataset '{DIM_DATASET}' erstellt.\")\n",
    "\n",
    "    # --- A) STATISCHE W√ñRTERB√úCHER ---\n",
    "    \n",
    "    # 1. Payment Type\n",
    "    sql_payment = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{dim_dataset_id}.dim_payment_type`\n",
    "    (payment_type_id INT64, payment_description STRING)\n",
    "    AS\n",
    "    SELECT * FROM UNNEST([\n",
    "        STRUCT(0 as payment_type_id, 'Flex / Unknown' as payment_description),\n",
    "        STRUCT(1, 'Credit Card'),\n",
    "        STRUCT(2, 'Cash'),\n",
    "        STRUCT(3, 'No Charge'),\n",
    "        STRUCT(4, 'Dispute'),\n",
    "        STRUCT(5, 'Unknown'),\n",
    "        STRUCT(6, 'Voided Trip')\n",
    "    ]);\n",
    "    \"\"\"\n",
    "\n",
    "    # 2. Rate Code\n",
    "    sql_ratecode = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{dim_dataset_id}.dim_rate_code`\n",
    "    (rate_code_id INT64, rate_description STRING)\n",
    "    AS\n",
    "    SELECT * FROM UNNEST([\n",
    "        STRUCT(1 as rate_code_id, 'Standard Rate' as rate_description),\n",
    "        STRUCT(2, 'JFK Airport'),\n",
    "        STRUCT(3, 'Newark Airport'),\n",
    "        STRUCT(4, 'Nassau/Westchester'),\n",
    "        STRUCT(5, 'Negotiated Fare'),\n",
    "        STRUCT(6, 'Group Ride'),\n",
    "        STRUCT(99, 'Unknown / FHV')\n",
    "    ]);\n",
    "    \"\"\"\n",
    "\n",
    "    # 3. Trip Type\n",
    "    sql_triptype = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{dim_dataset_id}.dim_trip_type`\n",
    "    (trip_type_id INT64, trip_description STRING)\n",
    "    AS\n",
    "    SELECT * FROM UNNEST([\n",
    "        STRUCT(1 as trip_type_id, 'Street-hail' as trip_description),\n",
    "        STRUCT(2, 'Dispatch')\n",
    "    ]);\n",
    "    \"\"\"\n",
    "\n",
    "    # --- B) VENDOR (REDUZIERT) ---\n",
    "    # Nur die offiziellen Taxi-Technologie-Partner. \n",
    "    # Wir lassen FHV-Basen komplett raus, wie gew√ºnscht.\n",
    "    sql_vendor = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{dim_dataset_id}.dim_vendor`\n",
    "    (vendor_id STRING, vendor_name STRING, source_system STRING)\n",
    "    AS\n",
    "    SELECT * FROM UNNEST([\n",
    "        STRUCT('1' as vendor_id, 'Creative Mobile Technologies (CMT)' as vendor_name, 'YELLOW/GREEN' as source_system),\n",
    "        STRUCT('2', 'VeriFone Inc. (Curb)', 'YELLOW/GREEN'),\n",
    "        STRUCT('6', 'Myle', 'GREEN'),\n",
    "        STRUCT('7', 'Helix', 'GREEN')\n",
    "    ]);\n",
    "    \"\"\"\n",
    "\n",
    "    # --- C) LOCATION (MAPS) ---\n",
    "    # Basiert auf Zelle 4a (GeoJSON Upload)\n",
    "    sql_location = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{dim_dataset_id}.dim_location`\n",
    "    (\n",
    "        LocationID INT64,\n",
    "        Borough STRING,\n",
    "        Zone STRING,\n",
    "        service_zone STRING,\n",
    "        zone_geom GEOGRAPHY\n",
    "    )\n",
    "    AS\n",
    "    SELECT \n",
    "        CAST(LocationID AS INT64) as LocationID,\n",
    "        Borough,\n",
    "        Zone,\n",
    "        service_zone,\n",
    "        ST_GEOGFROMGEOJSON(geometry_json, make_valid => TRUE) as zone_geom\n",
    "    FROM `{PROJECT_ID}.dimensional.staging_taxi_zones`\n",
    "    \"\"\"\n",
    "\n",
    "    # --- AUSF√úHREN ---\n",
    "    try:\n",
    "        client.query(sql_payment, location=CORRECT_LOCATION).result()\n",
    "        print(\"‚úÖ dim_payment_type erstellt.\")\n",
    "        \n",
    "        client.query(sql_ratecode, location=CORRECT_LOCATION).result()\n",
    "        print(\"‚úÖ dim_rate_code erstellt.\")\n",
    "        \n",
    "        client.query(sql_triptype, location=CORRECT_LOCATION).result()\n",
    "        print(\"‚úÖ dim_trip_type erstellt.\")\n",
    "        \n",
    "        client.query(sql_vendor, location=CORRECT_LOCATION).result()\n",
    "        print(\"‚úÖ dim_vendor erstellt (Nur Static Vendors).\")\n",
    "        \n",
    "        # Location nur erstellen, wenn Upload (Zelle 4a) gemacht wurde\n",
    "        try:\n",
    "            client.query(sql_location, location=CORRECT_LOCATION).result()\n",
    "            print(\"‚úÖ dim_location erstellt (Maps).\")\n",
    "        except Exception as loc_e:\n",
    "            print(f\"‚ö†Ô∏è Warnung: dim_location konnte nicht erstellt werden (Hast du Zelle 4a ausgef√ºhrt?). Fehler: {loc_e}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Genereller Fehler: {e}\")\n",
    "\n",
    "create_dimensional_layer_final()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13a99c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïµÔ∏è‚Äç‚ôÇÔ∏è STARTE QUALIT√ÑTS-CHECK (Canonical Layer)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lania/Documents/GitHub/bi_project_task2/.venv/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ERGEBNISSE ---\n",
      "                                  0\n",
      "total_rows                688622858\n",
      "clean_rows                685082821\n",
      "flagged_rows                3540037\n",
      "null_pickups                      0\n",
      "null_fares                515372815\n",
      "credit_card_zero_revenue      25021\n",
      "zero_distance_trips         2551622\n",
      "\n",
      "‚ùå WARNUNG: 515372815 kritische NULL-Werte gefunden!\n"
     ]
    }
   ],
   "source": [
    "# Zelle 8: Quality Audit (Der T√úV f√ºr deine Daten)\n",
    "def check_data_quality_stats():\n",
    "    print(\"üïµÔ∏è‚Äç‚ôÇÔ∏è STARTE QUALIT√ÑTS-CHECK (Canonical Layer)...\")\n",
    "    \n",
    "    sql_audit = f\"\"\"\n",
    "    SELECT \n",
    "        -- 1. Wie viele Fahrten haben wir insgesamt?\n",
    "        COUNT(*) as total_rows,\n",
    "        \n",
    "        -- 2. Wie viele sind \"Technisch Sauber\" (Kein DQ Flag)?\n",
    "        COUNTIF(dq_issue_flag = FALSE) as clean_rows,\n",
    "        \n",
    "        -- 3. Wie viele haben \"Business-Probleme\" (DQ Flag = TRUE)?\n",
    "        COUNTIF(dq_issue_flag = TRUE) as flagged_rows,\n",
    "        \n",
    "        -- 4. NULL CHECK (Darf eigentlich nicht sein, au√üer bei optionalen Feldern)\n",
    "        COUNTIF(pickup_datetime IS NULL) as null_pickups,\n",
    "        COUNTIF(fare_amount IS NULL) as null_fares,\n",
    "        \n",
    "        -- 5. LOGIK CHECK: Credit Card mit 0$ (Dein Sorgenkind)\n",
    "        COUNTIF(payment_type = 1 AND total_amount = 0) as credit_card_zero_revenue,\n",
    "        \n",
    "        -- 6. GEISTER-FAHRTEN (Keine Distanz)\n",
    "        COUNTIF(trip_distance = 0) as zero_distance_trips\n",
    "\n",
    "    FROM `{PROJECT_ID}.{TARGET_DATASET}.canonical_unified_taxi`\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        df = client.query(sql_audit).to_dataframe()\n",
    "        print(\"\\n--- ERGEBNISSE ---\")\n",
    "        print(df.T) # Transponieren f√ºr bessere Lesbarkeit\n",
    "        \n",
    "        # Automatische Bewertung\n",
    "        null_errors = df['null_fares'][0] + df['null_pickups'][0]\n",
    "        if null_errors == 0:\n",
    "            print(\"\\n‚úÖ TEST BESTANDEN: Keine technischen NULL-Werte in kritischen Spalten.\")\n",
    "        else:\n",
    "            print(f\"\\n‚ùå WARNUNG: {null_errors} kritische NULL-Werte gefunden!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fehler beim Audit: {e}\")\n",
    "\n",
    "check_data_quality_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b6c01c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lania/Documents/GitHub/bi_project_task2/.venv/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  source_system  total_fahrten  fahrten_ohne_preis  prozent_ohne_preis\n",
      "0        YELLOW      105211397                   0                 0.0\n",
      "1         GREEN       68038646                   0                 0.0\n",
      "2           FHV      515372815           515372815               100.0\n"
     ]
    }
   ],
   "source": [
    "# Zelle 9: Der \"Panic-Check\" - Woher kommen die NULLs?\n",
    "sql_debug = f\"\"\"\n",
    "SELECT \n",
    "    source_system,\n",
    "    COUNT(*) as total_fahrten,\n",
    "    COUNTIF(fare_amount IS NULL) as fahrten_ohne_preis,\n",
    "    ROUND(COUNTIF(fare_amount IS NULL) / COUNT(*) * 100, 2) as prozent_ohne_preis\n",
    "FROM `{PROJECT_ID}.{TARGET_DATASET}.canonical_unified_taxi`\n",
    "GROUP BY source_system\n",
    "\"\"\"\n",
    "print(client.query(sql_debug).to_dataframe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

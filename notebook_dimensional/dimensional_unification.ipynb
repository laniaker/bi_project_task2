{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d77b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sorgt daf√ºr, dass Plots im Notebook angezeigt werden\n",
    "%matplotlib inline\n",
    "\n",
    "# Lesbarkeit in der Exploration erh√∂hen\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "pd.set_option(\"display.width\", 140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c93bf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ QUELLE GEFUNDEN: Dataset 'dimensional' liegt in Region: 'EU'\n"
     ]
    }
   ],
   "source": [
    "# Zelle 1 & 2: Setup mit automatischer Regionen-Korrektur\n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import NotFound\n",
    "import logging\n",
    "import uuid\n",
    "import datetime\n",
    "\n",
    "# Logging Setup\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# --- CONFIG ---\n",
    "PROJECT_ID = \"taxi-bi-project\" # Deine ID aus dem Log\n",
    "DIM_DATASET = \"dimensional\"\n",
    "SOURCE_DATASET = \"staging\"      \n",
    "CAN_DATASET = \"canonical\"   \n",
    "\n",
    "# Tabellen\n",
    "TARGET_TABLE = \"dimensional\"\n",
    "\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "try:\n",
    "    src_ds_ref = client.get_dataset(f\"{PROJECT_ID}.{DIM_DATASET}\")\n",
    "    CORRECT_LOCATION = src_ds_ref.location\n",
    "    print(f\"‚úÖ QUELLE GEFUNDEN: Dataset '{DIM_DATASET}' liegt in Region: '{CORRECT_LOCATION}'\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå KRITISCHER FEHLER: Konnte Quell-Dataset '{DIM_DATASET}' nicht finden!\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5de23220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Lese GeoJSON 'NYC_Taxi_Zones.geojson' ---\n",
      "Gelesen: 263 Zonen.\n",
      "‚úÖ Tabelle 'taxi-bi-project.dimensional.staging_taxi_zones' erfolgreich hochgeladen.\n"
     ]
    }
   ],
   "source": [
    "# Zelle 4a: GeoJSON Upload (Direkt ins Dimensional Dataset)\n",
    "import json\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# CONFIG\n",
    "GEOJSON_FILE = \"NYC_Taxi_Zones.geojson\" \n",
    "# √ÑNDERUNG: Wir laden es direkt in das 'dimensional' Dataset\n",
    "STAGING_TABLE = f\"{PROJECT_ID}.dimensional.staging_taxi_zones\"\n",
    "\n",
    "def upload_geojson_to_bq():\n",
    "    print(f\"--- Lese GeoJSON '{GEOJSON_FILE}' ---\")\n",
    "    \n",
    "    try:\n",
    "        with open(GEOJSON_FILE, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Datei '{GEOJSON_FILE}' nicht gefunden. Bitte hochladen!\")\n",
    "        return\n",
    "\n",
    "    # Dataset 'dimensional' sicherstellen (falls Zelle 4a vor 4b l√§uft)\n",
    "    try:\n",
    "        client.get_dataset(f\"{PROJECT_ID}.dimensional\")\n",
    "    except:\n",
    "        new_ds = bigquery.Dataset(f\"{PROJECT_ID}.dimensional\")\n",
    "        new_ds.location = CORRECT_LOCATION\n",
    "        client.create_dataset(new_ds)\n",
    "        print(\"‚úÖ Dataset 'dimensional' erstellt.\")\n",
    "\n",
    "    rows = []\n",
    "    for feature in data['features']:\n",
    "        props = feature['properties']\n",
    "        geom = feature['geometry']\n",
    "        \n",
    "        # Flexibles Mapping\n",
    "        loc_id = props.get('LocationID') or props.get('objectid') or props.get('OBJECTID')\n",
    "        \n",
    "        row = {\n",
    "            'LocationID': loc_id,\n",
    "            'Zone': props.get('Zone') or props.get('zone'),\n",
    "            'Borough': props.get('Borough') or props.get('borough'),\n",
    "            'service_zone': props.get('service_zone'),\n",
    "            'geometry_json': json.dumps(geom)\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # FIX: Pandas zwingen, IDs als Zahl zu behandeln\n",
    "    df['LocationID'] = pd.to_numeric(df['LocationID'], errors='coerce').fillna(0).astype(int)\n",
    "    \n",
    "    print(f\"Gelesen: {len(df)} Zonen.\")\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=\"WRITE_TRUNCATE\",\n",
    "        schema=[\n",
    "            bigquery.SchemaField(\"LocationID\", \"INTEGER\"),\n",
    "            bigquery.SchemaField(\"Zone\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"Borough\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"service_zone\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"geometry_json\", \"STRING\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        job = client.load_table_from_dataframe(df, STAGING_TABLE, job_config=job_config)\n",
    "        job.result()\n",
    "        print(f\"‚úÖ Tabelle '{STAGING_TABLE}' erfolgreich hochgeladen.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Upload Fehler: {e}\")\n",
    "\n",
    "upload_geojson_to_bq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33531f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 4: Dimensional Layer (Komplett mit Smart-Base-Logik)\n",
    "def create_dimensional_layer_advanced():\n",
    "    dim_dataset_id = f\"{PROJECT_ID}.{DIM_DATASET}\"\n",
    "    \n",
    "    print(f\"--- Erstelle Advanced Layer 3 in '{DIM_DATASET}' ---\")\n",
    "\n",
    "    # Dataset erstellen\n",
    "    try:\n",
    "        client.get_dataset(dim_dataset_id)\n",
    "    except:\n",
    "        new_ds = bigquery.Dataset(dim_dataset_id)\n",
    "        new_ds.location = CORRECT_LOCATION\n",
    "        client.create_dataset(new_ds)\n",
    "\n",
    "    # 1. DIM: Payment & RateCode & TripType (Die einfachen Labels)\n",
    "    client.query(f\"CREATE OR REPLACE TABLE `{dim_dataset_id}.dim_payment_type` AS SELECT * FROM UNNEST([STRUCT(1 as payment_type_id, 'Credit Card' as payment_description), STRUCT(2, 'Cash'), STRUCT(0, 'Unknown')])\", location=CORRECT_LOCATION).result()\n",
    "    client.query(f\"CREATE OR REPLACE TABLE `{dim_dataset_id}.dim_rate_code` AS SELECT * FROM UNNEST([STRUCT(1 as rate_code_id, 'Standard' as rate_description), STRUCT(2, 'JFK'), STRUCT(99, 'Unknown')])\", location=CORRECT_LOCATION).result()\n",
    "    client.query(f\"CREATE OR REPLACE TABLE `{dim_dataset_id}.dim_trip_type` AS SELECT * FROM UNNEST([STRUCT(1 as trip_type_id, 'Street-hail' as trip_description), STRUCT(2, 'Dispatch')])\", location=CORRECT_LOCATION).result()\n",
    "\n",
    "    # 2. DIM: VENDOR \n",
    "    print(\"Erstelle dim_vendor mit Fallback-Logik f√ºr alte Basen...\")\n",
    "    \n",
    "    sql_vendor_smart = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{dim_dataset_id}.dim_vendor`\n",
    "    (vendor_id STRING, vendor_name STRING, source_system STRING, is_active BOOLEAN)\n",
    "    AS\n",
    "    WITH all_existing_bases AS (\n",
    "        -- 1. Wir schauen in die Vergangenheit: Wer ist alles gefahren?\n",
    "        SELECT DISTINCT dispatching_base_nummer as vid FROM `{PROJECT_ID}.{CAN_DATASET}.canonical_unified_taxi`\n",
    "        WHERE dispatching_base_nummer IS NOT NULL\n",
    "    ),\n",
    "    active_bases_list AS (\n",
    "        -- 2. Deine hochgeladene Datei (Nur die Aktiven)\n",
    "        -- PASS AUF DIE SPALTEN-NAMEN AUF! (Hier: license_number, name)\n",
    "        SELECT base_num, base_name FROM `{PROJECT_ID}.{SOURCE_DATASET}.staging_fhv_bases`\n",
    "    )\n",
    "    \n",
    "    SELECT \n",
    "        hist.vid as vendor_id,\n",
    "        -- Hier ist der \"Smart Logic\" Teil:\n",
    "        COALESCE(act.base_name, CONCAT('Inactive/Unknown Base ', hist.vid)) as vendor_name,\n",
    "        'FHV' as source_system,\n",
    "        IF(act.base_name IS NOT NULL, TRUE, FALSE) as is_active\n",
    "    FROM all_existing_bases hist\n",
    "    LEFT JOIN active_bases_list act ON hist.vid = act.base_num\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    -- Nicht vergessen: Die Yellow/Green Taxis\n",
    "    SELECT '1', 'Creative Mobile Technologies (CMT)', 'YELLOW/GREEN', TRUE\n",
    "    UNION ALL\n",
    "    SELECT '2', 'VeriFone Inc. (Curb)', 'YELLOW/GREEN', TRUE\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        client.query(sql_vendor_smart, location=CORRECT_LOCATION).result()\n",
    "        print(\"‚úÖ dim_vendor erstellt (Alte Basen wurden erhalten).\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Fehler bei dim_vendor (Hast du die Datei hochgeladen?): {e}\")\n",
    "\n",
    "    # 3. DIM: LOCATION (Optional - wenn du die Datei hast)\n",
    "    # Wenn du 'staging_taxi_zones' hochgeladen hast, nimm diesen Block:\n",
    "    \"\"\"\n",
    "    sql_location = f'''\n",
    "    CREATE OR REPLACE TABLE `{dim_dataset_id}.dim_location` AS\n",
    "    SELECT \n",
    "        LocationID, \n",
    "        Borough, \n",
    "        Zone, \n",
    "        service_zone \n",
    "    FROM `{PROJECT_ID}.{SOURCE_DATASET}.staging_taxi_zones`\n",
    "    '''\n",
    "    client.query(sql_location, location=CORRECT_LOCATION).result()\n",
    "    \"\"\"\n",
    "\n",
    "create_dimensional_layer_advanced()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5911943a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Erstelle Time Dimension: taxi-bi-project.dimensional.dim_date ---\n",
      "‚úÖ dim_date erfolgreich erstellt (2015-2026).\n"
     ]
    }
   ],
   "source": [
    "def create_dim_date():\n",
    "    DIM_DATASET = \"dimensional\"\n",
    "    dim_table_id = f\"{PROJECT_ID}.{DIM_DATASET}.dim_date\"\n",
    "    \n",
    "    print(f\"--- Erstelle Time Dimension: {dim_table_id} ---\")\n",
    "    \n",
    "    # Wir generieren Daten von 2015 bis Ende 2026\n",
    "    sql_date = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{dim_table_id}`\n",
    "    AS\n",
    "    SELECT\n",
    "        d AS date_key,  -- Primary Key f√ºr Joins\n",
    "        EXTRACT(YEAR FROM d) AS year,\n",
    "        EXTRACT(MONTH FROM d) AS month,\n",
    "        FORMAT_DATE('%B', d) AS month_name,  -- z.B. January\n",
    "        EXTRACT(WEEK FROM d) AS week_of_year,\n",
    "        EXTRACT(DAYOFWEEK FROM d) AS day_of_week_num, -- 1=Sunday in US, checke BI Tool Setting\n",
    "        FORMAT_DATE('%A', d) AS day_name,    -- z.B. Monday\n",
    "        EXTRACT(QUARTER FROM d) AS quarter,\n",
    "        \n",
    "        -- N√ºtzliche Flags f√ºr BI\n",
    "        CASE WHEN EXTRACT(DAYOFWEEK FROM d) IN (1, 7) THEN TRUE ELSE FALSE END AS is_weekend,\n",
    "        \n",
    "        -- Optional: Simple Saison-Logik\n",
    "        CASE \n",
    "            WHEN EXTRACT(MONTH FROM d) IN (12, 1, 2) THEN 'Winter'\n",
    "            WHEN EXTRACT(MONTH FROM d) IN (3, 4, 5) THEN 'Spring'\n",
    "            WHEN EXTRACT(MONTH FROM d) IN (6, 7, 8) THEN 'Summer'\n",
    "            ELSE 'Fall' \n",
    "        END AS season\n",
    "        \n",
    "    FROM UNNEST(GENERATE_DATE_ARRAY('2015-01-01', '2026-12-31')) AS d\n",
    "    ORDER BY d\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        client.query(sql_date).result()\n",
    "        print(\"‚úÖ dim_date erfolgreich erstellt (2015-2026).\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fehler bei dim_date: {e}\")\n",
    "\n",
    "# Ausf√ºhren\n",
    "create_dim_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df77383e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Erstelle Layer 3 (Dimensionen) in 'dimensional' ---\n",
      "‚úÖ dim_payment_type erstellt.\n",
      "‚úÖ dim_rate_code erstellt.\n",
      "‚úÖ dim_trip_type erstellt.\n",
      "‚úÖ dim_vendor erstellt (Nur Static Vendors).\n",
      "‚úÖ dim_location erstellt (Maps).\n"
     ]
    }
   ],
   "source": [
    "# Zelle 4: Dimensional Layer \n",
    "def create_dimensional_layer_final():\n",
    "    DIM_DATASET = \"dimensional\"\n",
    "    dim_dataset_id = f\"{PROJECT_ID}.{DIM_DATASET}\"\n",
    "    \n",
    "    print(f\"--- Erstelle Layer 3 (Dimensionen) in '{DIM_DATASET}' ---\")\n",
    "\n",
    "    # 1. Dataset anlegen\n",
    "    try:\n",
    "        client.get_dataset(dim_dataset_id)\n",
    "    except:\n",
    "        new_ds = bigquery.Dataset(dim_dataset_id)\n",
    "        new_ds.location = CORRECT_LOCATION\n",
    "        client.create_dataset(new_ds)\n",
    "        print(f\"‚úÖ Dataset '{DIM_DATASET}' erstellt.\")\n",
    "\n",
    "    # --- A) STATISCHE W√ñRTERB√úCHER ---\n",
    "    \n",
    "    # 1. Payment Type\n",
    "    sql_payment = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{dim_dataset_id}.dim_payment_type`\n",
    "    (payment_type_id INT64, payment_description STRING)\n",
    "    AS\n",
    "    SELECT * FROM UNNEST([\n",
    "        STRUCT(0 as payment_type_id, 'Flex / Unknown' as payment_description),\n",
    "        STRUCT(1, 'Credit Card'),\n",
    "        STRUCT(2, 'Cash'),\n",
    "        STRUCT(3, 'No Charge'),\n",
    "        STRUCT(4, 'Dispute'),\n",
    "        STRUCT(5, 'Unknown'),\n",
    "        STRUCT(6, 'Voided Trip')\n",
    "    ]);\n",
    "    \"\"\"\n",
    "\n",
    "    # 2. Rate Code\n",
    "    sql_ratecode = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{dim_dataset_id}.dim_rate_code`\n",
    "    (rate_code_id INT64, rate_description STRING)\n",
    "    AS\n",
    "    SELECT * FROM UNNEST([\n",
    "        STRUCT(1 as rate_code_id, 'Standard Rate' as rate_description),\n",
    "        STRUCT(2, 'JFK Airport'),\n",
    "        STRUCT(3, 'Newark Airport'),\n",
    "        STRUCT(4, 'Nassau/Westchester'),\n",
    "        STRUCT(5, 'Negotiated Fare'),\n",
    "        STRUCT(6, 'Group Ride'),\n",
    "        STRUCT(99, 'Unknown / FHV')\n",
    "    ]);\n",
    "    \"\"\"\n",
    "\n",
    "    # 3. Trip Type\n",
    "    sql_triptype = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{dim_dataset_id}.dim_trip_type`\n",
    "    (trip_type  _id INT64, trip_description STRING)\n",
    "    AS\n",
    "    SELECT * FROM UNNEST([\n",
    "        STRUCT(1 as trip_type_id, 'Street-hail' as trip_description),\n",
    "        STRUCT(2, 'Dispatch')\n",
    "    ]);\n",
    "    \"\"\"\n",
    "\n",
    "    # --- C) LOCATION (MAPS) ---\n",
    "    # Basiert auf Zelle 4a (GeoJSON Upload)\n",
    "    sql_location = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{dim_dataset_id}.dim_location`\n",
    "    (\n",
    "        LocationID INT64,\n",
    "        Borough STRING,\n",
    "        Zone STRING,\n",
    "        service_zone STRING,\n",
    "        zone_geom GEOGRAPHY\n",
    "    )\n",
    "    AS\n",
    "    SELECT \n",
    "        CAST(LocationID AS INT64) as LocationID,\n",
    "        Borough,\n",
    "        Zone,\n",
    "        service_zone,\n",
    "        ST_GEOGFROMGEOJSON(geometry_json, make_valid => TRUE) as zone_geom\n",
    "    FROM `{PROJECT_ID}.dimensional.staging_taxi_zones`\n",
    "    \"\"\"\n",
    "\n",
    "    # --- AUSF√úHREN ---\n",
    "    try:\n",
    "        client.query(sql_payment, location=CORRECT_LOCATION).result()\n",
    "        print(\"‚úÖ dim_payment_type erstellt.\")\n",
    "        \n",
    "        client.query(sql_ratecode, location=CORRECT_LOCATION).result()\n",
    "        print(\"‚úÖ dim_rate_code erstellt.\")\n",
    "        \n",
    "        client.query(sql_triptype, location=CORRECT_LOCATION).result()\n",
    "        print(\"‚úÖ dim_trip_type erstellt.\")\n",
    "        \n",
    "        # Location nur erstellen, wenn Upload (Zelle 4a) gemacht wurde\n",
    "        try:\n",
    "            client.query(sql_location, location=CORRECT_LOCATION).result()\n",
    "            print(\"‚úÖ dim_location erstellt (Maps).\")\n",
    "        except Exception as loc_e:\n",
    "            print(f\"‚ö†Ô∏è Warnung: dim_location konnte nicht erstellt werden (Hast du Zelle 4a ausgef√ºhrt?). Fehler: {loc_e}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Genereller Fehler: {e}\")\n",
    "\n",
    "create_dimensional_layer_final()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7867a47b",
   "metadata": {},
   "source": [
    "** Create Dimensional **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3fb5d0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Erstelle Optimized Fact Table: taxi-bi-project.dimensional.Fact_Trips ---\n",
      "‚úÖ Fact_Trips erfolgreich erstellt und partitioniert.\n"
     ]
    }
   ],
   "source": [
    "def create_fact_layer_optimized():\n",
    "    DIM_DATASET = \"dimensional\"\n",
    "    CAN_DATASET = \"canonical\"\n",
    "    \n",
    "    # Referenzen\n",
    "    source_table = f\"{PROJECT_ID}.{CAN_DATASET}.canonical_unified_taxi\"\n",
    "    fact_table = f\"{PROJECT_ID}.{DIM_DATASET}.Fact_Trips\"\n",
    "    \n",
    "    print(f\"--- Erstelle Optimized Fact Table: {fact_table} ---\")\n",
    "\n",
    "    sql_fact = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{fact_table}`\n",
    "    -- Partitionierung ist wichtig f√ºr Performance & Kosten!\n",
    "    PARTITION BY pickup_date_key\n",
    "    -- Cluster sortiert die Daten physisch f√ºr schnellere Filter\n",
    "    CLUSTER BY vendor_id, source_system\n",
    "    AS\n",
    "    SELECT\n",
    "        -- IDs\n",
    "        t.trip_id,\n",
    "        t.vendor_id,    -- Link zu dim_vendor\n",
    "        \n",
    "        -- DATE KEY (Der Link zu dim_date!)\n",
    "        DATE(t.pickup_datetime) AS pickup_date_key, \n",
    "        \n",
    "        -- Locations (Link zu dim_location)\n",
    "        COALESCE(t.PULocationID, 264) AS pickup_location_id,   -- 264 = Unknown (NV)\n",
    "        COALESCE(t.DOLocationID, 264) AS dropoff_location_id,\n",
    "        \n",
    "        -- Payment (Link zu dim_payment_type)\n",
    "        -- FHV hat oft 0, was wir in dim_payment als \"Unknown\" definiert haben. Passt.\n",
    "        IFNULL(t.payment_type, 0) AS payment_type_id,\n",
    "        \n",
    "        -- Rate Code (Link zu dim_rate_code)\n",
    "        IFNULL(t.RatecodeID, 99) AS rate_code_id,\n",
    "        \n",
    "        -- Trip Type (Link zu dim_trip_type)\n",
    "        t.Trip_type AS trip_type_id,\n",
    "        \n",
    "        -- Degenerate Dimensions (Werte, die direkt in der Faktentabelle bleiben)\n",
    "        t.source_system,\n",
    "        t.store_and_fwd_flag,\n",
    "        t.pickup_datetime, -- Behalten f√ºr genaue Zeitberechnung (Stunden/Minuten)\n",
    "        t.dropoff_datetime,\n",
    "        \n",
    "        -- Measures (Zahlenwerte)\n",
    "        -- Wir nutzen COALESCE(x, 0), damit Summen in Dashboards funktionieren\n",
    "        COALESCE(t.passenger_count, 0) AS passenger_count,\n",
    "        COALESCE(t.trip_distance, 0) AS trip_distance,\n",
    "        \n",
    "        -- Finanzen (Wichtig: FHV hat hier oft NULL, wir machen 0 daraus f√ºr Berechnungen)\n",
    "        COALESCE(t.fare_amount, 0) AS fare_amount,\n",
    "        COALESCE(t.tip_amount, 0) AS tip_amount,\n",
    "        COALESCE(t.tolls_amount, 0) AS tolls_amount,\n",
    "        COALESCE(t.extra, 0) AS extra,\n",
    "        COALESCE(t.mta_tax, 0) AS mta_tax,\n",
    "        COALESCE(t.improvement_surcharge, 0) AS improvement_surcharge,\n",
    "        COALESCE(t.congestion_surcharge, 0) AS congestion_surcharge,\n",
    "        COALESCE(t.total_amount, 0) AS total_amount,\n",
    "        \n",
    "        -- Berechnete Measures\n",
    "        TIMESTAMP_DIFF(t.dropoff_datetime, t.pickup_datetime, MINUTE) AS duration_minutes\n",
    "\n",
    "    FROM `{source_table}` t\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # dry_run=False ist Standard, aber gut zu wissen\n",
    "        job_config = bigquery.QueryJobConfig()\n",
    "        client.query(sql_fact, job_config=job_config).result()\n",
    "        print(\"‚úÖ Fact_Trips erfolgreich erstellt und partitioniert.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fehler bei Fact_Trips: {e}\")\n",
    "\n",
    "create_fact_layer_optimized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef9032ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üîç QA CHECK: Missing Location IDs by System ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lania/Documents/GitHub/bi_project_task2/.venv/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_system  total_trips  pu_is_null  do_is_null  pu_is_unknown  do_is_unknown  pct_bad_pickup\n",
      "          FHV    435300973   119085087    38701963       25732302       51104279            33.3\n",
      "        GREEN     55595206           0           0          58820         182338             0.1\n",
      "       YELLOW     53094835           0           0         516584         612980             1.0\n",
      "\n",
      "--- ANALYSE ---\n",
      "‚úÖ FHV Daten sehen gut aus (33.3% Missing).\n"
     ]
    }
   ],
   "source": [
    "# Zelle 7: QA Check - Sind FHV Locations da?\n",
    "def check_fhv_locations():\n",
    "    print(\"--- üîç QA CHECK: Missing Location IDs by System ---\")\n",
    "    \n",
    "    # Wir pr√ºfen Fact_Trips, da dies die Basis f√ºr deine Analysen ist\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        source_system,\n",
    "        COUNT(*) as total_trips,\n",
    "        \n",
    "        -- 1. Check auf ECHTE NULL-Werte\n",
    "        COUNTIF(pickup_location_id IS NULL) as pu_is_null,\n",
    "        COUNTIF(dropoff_location_id IS NULL) as do_is_null,\n",
    "        \n",
    "        -- 2. Check auf 'Unknown' IDs (264=NV, 265=NA)\n",
    "        -- Das ist bei FHV oft der Fall!\n",
    "        COUNTIF(pickup_location_id IN (264, 265)) as pu_is_unknown,\n",
    "        COUNTIF(dropoff_location_id IN (264, 265)) as do_is_unknown,\n",
    "        \n",
    "        -- Prozentualer Anteil (NULL + Unknown)\n",
    "        ROUND((COUNTIF(pickup_location_id IS NULL OR pickup_location_id IN (264, 265)) / COUNT(*)) * 100, 1) as pct_bad_pickup\n",
    "        \n",
    "    FROM `{PROJECT_ID}.dimensional.Fact_Trips`\n",
    "    GROUP BY source_system\n",
    "    ORDER BY source_system\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        df = client.query(query).to_dataframe()\n",
    "        print(df.to_string(index=False))\n",
    "        \n",
    "        # Kurze Analyse f√ºr dich ausgeben\n",
    "        print(\"\\n--- ANALYSE ---\")\n",
    "        fhv_row = df[df['source_system'] == 'FHV']\n",
    "        if not fhv_row.empty:\n",
    "            missing_pct = fhv_row.iloc[0]['pct_bad_pickup']\n",
    "            if missing_pct > 50:\n",
    "                print(f\"‚ö†Ô∏è ACHTUNG: {missing_pct}% der FHV-Trips haben keine g√ºltige Location!\")\n",
    "                print(\"   -> Das ist normal f√ºr √§ltere FHV-Daten, schr√§nkt aber Heatmaps ein.\")\n",
    "            else:\n",
    "                print(f\"‚úÖ FHV Daten sehen gut aus ({missing_pct}% Missing).\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fehler bei der Abfrage: {e}\")\n",
    "\n",
    "check_fhv_locations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13a99c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïµÔ∏è‚Äç‚ôÇÔ∏è STARTE QUALIT√ÑTS-CHECK (Canonical Layer)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lania/Documents/GitHub/bi_project_task2/.venv/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ERGEBNISSE ---\n",
      "                                  0\n",
      "total_rows                543991014\n",
      "null_pickups                      0\n",
      "null_fares                435300973\n",
      "credit_card_zero_revenue          2\n",
      "zero_distance_trips               0\n",
      "\n",
      "‚ùå WARNUNG: 435300973 kritische NULL-Werte gefunden!\n"
     ]
    }
   ],
   "source": [
    "# Zelle 8: Quality Audit (Der T√úV f√ºr deine Daten)\n",
    "def check_data_quality_stats():\n",
    "    print(\"üïµÔ∏è‚Äç‚ôÇÔ∏è STARTE QUALIT√ÑTS-CHECK (Canonical Layer)...\")\n",
    "    \n",
    "    sql_audit = f\"\"\"\n",
    "    SELECT \n",
    "        -- 1. Wie viele Fahrten haben wir insgesamt?\n",
    "        COUNT(*) as total_rows,\n",
    "    \n",
    "        -- 4. NULL CHECK (Darf eigentlich nicht sein, au√üer bei optionalen Feldern)\n",
    "        COUNTIF(pickup_datetime IS NULL) as null_pickups,\n",
    "        COUNTIF(fare_amount IS NULL) as null_fares,\n",
    "        \n",
    "        -- 5. LOGIK CHECK: Credit Card mit 0$ (Dein Sorgenkind)\n",
    "        COUNTIF(payment_type = 1 AND total_amount = 0) as credit_card_zero_revenue,\n",
    "        \n",
    "        -- 6. GEISTER-FAHRTEN (Keine Distanz)\n",
    "        COUNTIF(trip_distance = 0) as zero_distance_trips\n",
    "\n",
    "    FROM `{PROJECT_ID}.{CAN_DATASET}.canonical_unified_taxi`\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        df = client.query(sql_audit).to_dataframe()\n",
    "        print(\"\\n--- ERGEBNISSE ---\")\n",
    "        print(df.T) # Transponieren f√ºr bessere Lesbarkeit\n",
    "        \n",
    "        # Automatische Bewertung\n",
    "        null_errors = df['null_fares'][0] + df['null_pickups'][0]\n",
    "        if null_errors == 0:\n",
    "            print(\"\\n‚úÖ TEST BESTANDEN: Keine technischen NULL-Werte in kritischen Spalten.\")\n",
    "        else:\n",
    "            print(f\"\\n‚ùå WARNUNG: {null_errors} kritische NULL-Werte gefunden!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fehler beim Audit: {e}\")\n",
    "\n",
    "check_data_quality_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b6c01c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  source_system  total_fahrten  fahrten_ohne_preis  prozent_ohne_preis\n",
      "0        YELLOW       53094835                   0                 0.0\n",
      "1           FHV      435300973           435300973               100.0\n",
      "2         GREEN       55595206                   0                 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lania/Documents/GitHub/bi_project_task2/.venv/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Zelle 9: Der \"Panic-Check\" - Woher kommen die NULLs?\n",
    "sql_debug = f\"\"\"\n",
    "SELECT \n",
    "    source_system,\n",
    "    COUNT(*) as total_fahrten,\n",
    "    COUNTIF(fare_amount IS NULL) as fahrten_ohne_preis,\n",
    "    ROUND(COUNTIF(fare_amount IS NULL) / COUNT(*) * 100, 2) as prozent_ohne_preis\n",
    "FROM `{PROJECT_ID}.{CAN_DATASET}.canonical_unified_taxi`\n",
    "GROUP BY source_system\n",
    "\"\"\"\n",
    "print(client.query(sql_debug).to_dataframe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d77b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sorgt daf√ºr, dass Plots im Notebook angezeigt werden\n",
    "%matplotlib inline\n",
    "\n",
    "# Lesbarkeit in der Exploration erh√∂hen\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "pd.set_option(\"display.width\", 140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c93bf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ QUELLE GEFUNDEN: Dataset 'dimensional' liegt in Region: 'EU'\n"
     ]
    }
   ],
   "source": [
    "# Zelle 1 & 2: Setup mit automatischer Regionen-Korrektur\n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import NotFound\n",
    "import logging\n",
    "import uuid\n",
    "import datetime\n",
    "\n",
    "# Logging Setup\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# --- CONFIG ---\n",
    "PROJECT_ID = \"taxi-bi-project\" # Deine ID aus dem Log\n",
    "DIM_DATASET = \"dimensional\"\n",
    "SOURCE_DATASET = \"staging\"      \n",
    "CAN_DATASET = \"canonical\"   \n",
    "\n",
    "# Tabellen\n",
    "TARGET_TABLE = \"dimensional\"\n",
    "\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "try:\n",
    "    src_ds_ref = client.get_dataset(f\"{PROJECT_ID}.{DIM_DATASET}\")\n",
    "    CORRECT_LOCATION = src_ds_ref.location\n",
    "    print(f\"‚úÖ QUELLE GEFUNDEN: Dataset '{DIM_DATASET}' liegt in Region: '{CORRECT_LOCATION}'\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå KRITISCHER FEHLER: Konnte Quell-Dataset '{DIM_DATASET}' nicht finden!\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33531f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 4: Dimensional Layer (Komplett mit Smart-Base-Logik)\n",
    "def create_dimensional_layer_advanced():\n",
    "    dim_dataset_id = f\"{PROJECT_ID}.{DIM_DATASET}\"\n",
    "    \n",
    "    print(f\"--- Erstelle Advanced Layer 3 in '{DIM_DATASET}' ---\")\n",
    "\n",
    "    # Dataset erstellen\n",
    "    try:\n",
    "        client.get_dataset(dim_dataset_id)\n",
    "    except:\n",
    "        new_ds = bigquery.Dataset(dim_dataset_id)\n",
    "        new_ds.location = CORRECT_LOCATION\n",
    "        client.create_dataset(new_ds)\n",
    "\n",
    "    # 1. DIM: Payment & RateCode & TripType (Die einfachen Labels)\n",
    "    client.query(f\"CREATE OR REPLACE TABLE `{dim_dataset_id}.dim_payment_type` AS SELECT * FROM UNNEST([STRUCT(1 as payment_type_id, 'Credit Card' as payment_description), STRUCT(2, 'Cash'), STRUCT(0, 'Unknown')])\", location=CORRECT_LOCATION).result()\n",
    "    client.query(f\"CREATE OR REPLACE TABLE `{dim_dataset_id}.dim_rate_code` AS SELECT * FROM UNNEST([STRUCT(1 as rate_code_id, 'Standard' as rate_description), STRUCT(2, 'JFK'), STRUCT(99, 'Unknown')])\", location=CORRECT_LOCATION).result()\n",
    "    client.query(f\"CREATE OR REPLACE TABLE `{dim_dataset_id}.dim_trip_type` AS SELECT * FROM UNNEST([STRUCT(1 as trip_type_id, 'Street-hail' as trip_description), STRUCT(2, 'Dispatch')])\", location=CORRECT_LOCATION).result()\n",
    "\n",
    "    # 2. DIM: VENDOR \n",
    "    print(\"Erstelle dim_vendor mit Fallback-Logik f√ºr alte Basen...\")\n",
    "    \n",
    "    sql_vendor_smart = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{dim_dataset_id}.dim_vendor`\n",
    "    (vendor_id STRING, vendor_name STRING, source_system STRING, is_active BOOLEAN)\n",
    "    AS\n",
    "    WITH all_existing_bases AS (\n",
    "        -- 1. Wir schauen in die Vergangenheit: Wer ist alles gefahren?\n",
    "        SELECT DISTINCT dispatching_base_nummer as vid FROM `{PROJECT_ID}.{CAN_DATASET}.canonical_unified_taxi`\n",
    "        WHERE dispatching_base_nummer IS NOT NULL\n",
    "    ),\n",
    "    active_bases_list AS (\n",
    "        -- 2. Deine hochgeladene Datei (Nur die Aktiven)\n",
    "        -- PASS AUF DIE SPALTEN-NAMEN AUF! (Hier: license_number, name)\n",
    "        SELECT base_num, base_name FROM `{PROJECT_ID}.{SOURCE_DATASET}.staging_fhv_bases`\n",
    "    )\n",
    "    \n",
    "    SELECT \n",
    "        hist.vid as vendor_id,\n",
    "        -- Hier ist der \"Smart Logic\" Teil:\n",
    "        COALESCE(act.base_name, CONCAT('Inactive/Unknown Base ', hist.vid)) as vendor_name,\n",
    "        'FHV' as source_system,\n",
    "        IF(act.base_name IS NOT NULL, TRUE, FALSE) as is_active\n",
    "    FROM all_existing_bases hist\n",
    "    LEFT JOIN active_bases_list act ON hist.vid = act.base_num\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    -- Nicht vergessen: Die Yellow/Green Taxis\n",
    "    SELECT '1', 'Creative Mobile Technologies (CMT)', 'YELLOW/GREEN', TRUE\n",
    "    UNION ALL\n",
    "    SELECT '2', 'VeriFone Inc. (Curb)', 'YELLOW/GREEN', TRUE\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        client.query(sql_vendor_smart, location=CORRECT_LOCATION).result()\n",
    "        print(\"‚úÖ dim_vendor erstellt (Alte Basen wurden erhalten).\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Fehler bei dim_vendor (Hast du die Datei hochgeladen?): {e}\")\n",
    "\n",
    "    # 3. DIM: LOCATION (Optional - wenn du die Datei hast)\n",
    "    # Wenn du 'staging_taxi_zones' hochgeladen hast, nimm diesen Block:\n",
    "    \"\"\"\n",
    "    sql_location = f'''\n",
    "    CREATE OR REPLACE TABLE `{dim_dataset_id}.dim_location` AS\n",
    "    SELECT \n",
    "        LocationID, \n",
    "        Borough, \n",
    "        Zone, \n",
    "        service_zone \n",
    "    FROM `{PROJECT_ID}.{SOURCE_DATASET}.staging_taxi_zones`\n",
    "    '''\n",
    "    client.query(sql_location, location=CORRECT_LOCATION).result()\n",
    "    \"\"\"\n",
    "\n",
    "create_dimensional_layer_advanced()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5911943a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Erstelle Time Dimension: taxi-bi-project.dimensional.dim_date ---\n",
      "‚úÖ dim_date erfolgreich erstellt (2015-2026).\n"
     ]
    }
   ],
   "source": [
    "def create_dim_date():\n",
    "    DIM_DATASET = \"dimensional\"\n",
    "    dim_table_id = f\"{PROJECT_ID}.{DIM_DATASET}.dim_date\"\n",
    "    \n",
    "    print(f\"--- Erstelle Time Dimension: {dim_table_id} ---\")\n",
    "    \n",
    "    # Wir generieren Daten von 2015 bis Ende 2026\n",
    "    sql_date = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{dim_table_id}`\n",
    "    AS\n",
    "    SELECT\n",
    "        d AS date_key,  -- Primary Key f√ºr Joins\n",
    "        EXTRACT(YEAR FROM d) AS year,\n",
    "        EXTRACT(MONTH FROM d) AS month,\n",
    "        FORMAT_DATE('%B', d) AS month_name,  -- z.B. January\n",
    "        EXTRACT(WEEK FROM d) AS week_of_year,\n",
    "        EXTRACT(DAYOFWEEK FROM d) AS day_of_week_num, -- 1=Sunday in US, checke BI Tool Setting\n",
    "        FORMAT_DATE('%A', d) AS day_name,    -- z.B. Monday\n",
    "        EXTRACT(QUARTER FROM d) AS quarter,\n",
    "        \n",
    "        -- N√ºtzliche Flags f√ºr BI\n",
    "        CASE WHEN EXTRACT(DAYOFWEEK FROM d) IN (1, 7) THEN TRUE ELSE FALSE END AS is_weekend,\n",
    "        \n",
    "        -- Optional: Simple Saison-Logik\n",
    "        CASE \n",
    "            WHEN EXTRACT(MONTH FROM d) IN (12, 1, 2) THEN 'Winter'\n",
    "            WHEN EXTRACT(MONTH FROM d) IN (3, 4, 5) THEN 'Spring'\n",
    "            WHEN EXTRACT(MONTH FROM d) IN (6, 7, 8) THEN 'Summer'\n",
    "            ELSE 'Fall' \n",
    "        END AS season\n",
    "        \n",
    "    FROM UNNEST(GENERATE_DATE_ARRAY('2015-01-01', '2026-12-31')) AS d\n",
    "    ORDER BY d\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        client.query(sql_date).result()\n",
    "        print(\"‚úÖ dim_date erfolgreich erstellt (2015-2026).\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fehler bei dim_date: {e}\")\n",
    "\n",
    "# Ausf√ºhren\n",
    "create_dim_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df77383e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Erstelle Layer 3 (Dimensionen) in 'dimensional' ---\n",
      "‚úÖ Alle Dimensionstabellen (Vendor, Payment, Rate, Type, Shared, Location) wurden erstellt.\n"
     ]
    }
   ],
   "source": [
    "# Zelle 4: Dimensional Layer (Das \"W√∂rterbuch\" f√ºr dein Dashboard)\n",
    "def create_dimensional_layer_final():\n",
    "    DIM_DATASET = \"dimensional\"\n",
    "    dim_dataset_id = f\"{PROJECT_ID}.{DIM_DATASET}\"\n",
    "    \n",
    "    print(f\"--- Erstelle Layer 3 (Dimensionen) in '{DIM_DATASET}' ---\")\n",
    "\n",
    "    # 1. Dataset anlegen falls nicht vorhanden\n",
    "    try:\n",
    "        client.get_dataset(dim_dataset_id)\n",
    "    except:\n",
    "        new_ds = bigquery.Dataset(dim_dataset_id)\n",
    "        new_ds.location = CORRECT_LOCATION\n",
    "        client.create_dataset(new_ds)\n",
    "        print(f\"‚úÖ Dataset '{DIM_DATASET}' erstellt.\")\n",
    "\n",
    "    # --- A) DIM VENDOR (Mapping f√ºr vid) ---\n",
    "    sql_vendor = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{dim_dataset_id}.dim_vendor` AS\n",
    "    SELECT * FROM UNNEST([\n",
    "        STRUCT('1' as vendor_id, 'Creative Mobile (CMT)' as vendor_name),\n",
    "        STRUCT('2', 'VeriFone (VTS/Curb)'),\n",
    "        STRUCT('3', 'Digital Dispatch (DDS)'),\n",
    "        STRUCT('6', 'Myle'),\n",
    "        STRUCT('7', 'Helix'),\n",
    "        STRUCT('99', 'Unknown / Dispatch Base')\n",
    "    ]);\n",
    "    \"\"\"\n",
    "\n",
    "    # --- B) DIM PAYMENT TYPE (Inkl. deiner Rettung aus 2018) ---\n",
    "    sql_payment = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{dim_dataset_id}.dim_payment_type` AS\n",
    "    SELECT * FROM UNNEST([\n",
    "        STRUCT(0 as payment_type_id, 'App-Payment / Flex (FHV)' as payment_description),\n",
    "        STRUCT(1, 'Credit Card'),\n",
    "        STRUCT(2, 'Cash'),\n",
    "        STRUCT(3, 'No Charge'),\n",
    "        STRUCT(4, 'Dispute'),\n",
    "        STRUCT(5, 'Unknown / Rescued (2018 Fix)'),\n",
    "        STRUCT(6, 'Voided Trip')\n",
    "    ]);\n",
    "    \"\"\"\n",
    "\n",
    "    # --- C) DIM RATE CODE ---\n",
    "    sql_ratecode = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{dim_dataset_id}.dim_rate_code` AS\n",
    "    SELECT * FROM UNNEST([\n",
    "        STRUCT(1 as rate_code_id, 'Standard Rate' as rate_description),\n",
    "        STRUCT(2, 'JFK Airport'),\n",
    "        STRUCT(3, 'Newark Airport'),\n",
    "        STRUCT(4, 'Nassau/Westchester'),\n",
    "        STRUCT(5, 'Negotiated Fare'),\n",
    "        STRUCT(6, 'Group Ride'),\n",
    "        STRUCT(99, 'Unknown / FHV')\n",
    "    ]);\n",
    "    \"\"\"\n",
    "\n",
    "    # --- D) DIM TRIP TYPE ---\n",
    "    sql_triptype = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{dim_dataset_id}.dim_trip_type` AS\n",
    "    SELECT * FROM UNNEST([\n",
    "        STRUCT(1 as trip_type_id, 'Street-hail (Yellow/Green)' as trip_description),\n",
    "        STRUCT(2, 'Dispatch (FHV/Uber/Lyft)' as trip_description)\n",
    "    ]);\n",
    "    \"\"\"\n",
    "\n",
    "    # --- E) DIM SHARED RIDE (Wichtig f√ºr deinen Hinweis zu Lyft) ---\n",
    "    sql_shared = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{dim_dataset_id}.dim_shared_ride` AS\n",
    "    SELECT * FROM UNNEST([\n",
    "        STRUCT(TRUE as sr_flag, 'Shared Ride' as sr_description, 'Note: Lyft counts requests, others only matches.' as sr_note),\n",
    "        STRUCT(FALSE, 'Private Ride', 'Standard single passenger request.')\n",
    "    ]);\n",
    "    \"\"\"\n",
    "\n",
    "    # --- F) DIM LOCATION (Zonen-Mapping) ---\n",
    "    sql_location = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{dim_dataset_id}.dim_location` AS\n",
    "    SELECT \n",
    "        CAST(LocationID AS INT64) as location_id,\n",
    "        Borough as borough,\n",
    "        Zone as zone,\n",
    "        service_zone,\n",
    "        ST_GEOGFROMGEOJSON(geometry_json, make_valid => TRUE) as zone_geom\n",
    "    FROM `{PROJECT_ID}.dimensional.taxi_zones`\n",
    "    \"\"\"\n",
    "\n",
    "    # --- AUSF√úHREN ---\n",
    "    try:\n",
    "        client.query(sql_vendor).result()\n",
    "        client.query(sql_payment).result()\n",
    "        client.query(sql_ratecode).result()\n",
    "        client.query(sql_triptype).result()\n",
    "        client.query(sql_shared).result()\n",
    "        client.query(sql_location).result()\n",
    "        print(\"‚úÖ Alle Dimensionstabellen (Vendor, Payment, Rate, Type, Shared, Location) wurden erstellt.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fehler beim Erstellen der Dimensionen: {e}\")\n",
    "\n",
    "create_dimensional_layer_final()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7867a47b",
   "metadata": {},
   "source": [
    "** Create Dimensional **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fb5d0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Erstelle/Update Optimized Fact Table: taxi-bi-project.dimensional.Fact_Trips ---\n",
      "üóëÔ∏è Alte Tabellen-Struktur gel√∂scht.\n",
      "‚úÖ fact_trips erfolgreich mit monatlicher Partitionierung erstellt.\n"
     ]
    }
   ],
   "source": [
    "def create_fact_layer_optimized_v2():\n",
    "    DIM_DATASET = \"dimensional\"\n",
    "    CAN_DATASET = \"canonical\"\n",
    "    \n",
    "    # Referenzen\n",
    "    source_table = f\"{PROJECT_ID}.{CAN_DATASET}.canonical_unified_taxi\"\n",
    "    fact_table = f\"{PROJECT_ID}.{DIM_DATASET}.Fact_Trips\"\n",
    "    print(f\"--- Erstelle/Update Optimized Fact Table: {fact_table} ---\")\n",
    "\n",
    "    sql_drop = f\"DROP TABLE IF EXISTS `{fact_table}`\"\n",
    "    \n",
    "    sql_fact = f\"\"\"\n",
    "    CREATE TABLE `{fact_table}`\n",
    "    -- Monatliche Partitionierung, um das 4000er Limit zu umgehen\n",
    "    PARTITION BY DATETIME_TRUNC(pickup_datetime, MONTH)\n",
    "    CLUSTER BY vendor_id, source_system\n",
    "    AS\n",
    "    SELECT\n",
    "        -- IDs & Links\n",
    "        t.trip_id,\n",
    "        t.vendor_id,\n",
    "        t.dispatching_base_nummer,\n",
    "        \n",
    "        -- DATE & TIME\n",
    "        DATE(t.pickup_datetime) AS pickup_date_key, \n",
    "        t.pickup_datetime,\n",
    "        t.dropoff_datetime,\n",
    "        \n",
    "        -- Locations\n",
    "        COALESCE(t.pickup_location_id, 263) AS pickup_location_id, \n",
    "        COALESCE(t.dropoff_location_id, 263) AS dropoff_location_id,\n",
    "        \n",
    "        -- Payment & Rate\n",
    "        IFNULL(t.payment_type, 0) AS payment_type_id,\n",
    "        IFNULL(t.RatecodeID, 99) AS rate_code_id,\n",
    "        \n",
    "        -- Trip Type & Shared Ride\n",
    "        t.Trip_type AS trip_type_id,\n",
    "        IFNULL(t.SR_Flag, FALSE) AS sr_flag,\n",
    "        \n",
    "        -- System Info\n",
    "        t.source_system,\n",
    "        t.store_and_fwd_flag,\n",
    "        t.dq_issue_flag,\n",
    "        \n",
    "        -- Measures\n",
    "        COALESCE(t.passenger_count, 0) AS passenger_count,\n",
    "        COALESCE(t.trip_distance, 0) AS trip_distance,\n",
    "        COALESCE(t.fare_amount, 0) AS fare_amount,\n",
    "        COALESCE(t.tip_amount, 0) AS tip_amount,\n",
    "        COALESCE(t.total_amount, 0) AS total_amount,\n",
    "        COALESCE(t.congestion_surcharge, 0) AS congestion_surcharge,\n",
    "        COALESCE(t.Airport_fee, 0) AS airport_fee,\n",
    "        \n",
    "        -- Berechnete Measures\n",
    "        TIMESTAMP_DIFF(t.dropoff_datetime, t.pickup_datetime, MINUTE) AS duration_minutes\n",
    "\n",
    "    FROM `{source_table}` t\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Erst l√∂schen\n",
    "        client.query(sql_drop).result()\n",
    "        print(\"üóëÔ∏è Alte Tabellen-Struktur gel√∂scht.\")\n",
    "        \n",
    "        # Dann neu erstellen\n",
    "        client.query(sql_fact).result()\n",
    "        print(\"‚úÖ fact_trips erfolgreich mit monatlicher Partitionierung erstellt.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fehler: {e}\")\n",
    "\n",
    "create_fact_layer_optimized_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef9032ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üîç QA CHECK: Missing Location IDs by System ---\n",
      "source_system  total_trips  pu_is_null  do_is_null  pu_is_unknown  do_is_unknown  pct_bad_pickup\n",
      "          FHV    514630379           0           0       26673183       55183257             5.2\n",
      "        GREEN     67447832           0           0         115965         250751             0.2\n",
      "       YELLOW    172066569           0           0        2366782        2612666             1.4\n",
      "\n",
      "--- ANALYSE ---\n",
      "‚úÖ FHV Daten sehen gut aus (5.2% Missing).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lania/Documents/GitHub/bi_project_task2/.venv/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Zelle 7: QA Check - Sind FHV Locations da?\n",
    "def check_fhv_locations():\n",
    "    print(\"--- üîç QA CHECK: Missing Location IDs by System ---\")\n",
    "    \n",
    "    # Wir pr√ºfen Fact_Trips, da dies die Basis f√ºr deine Analysen ist\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        source_system,\n",
    "        COUNT(*) as total_trips,\n",
    "        \n",
    "        -- 1. Check auf ECHTE NULL-Werte\n",
    "        COUNTIF(pickup_location_id IS NULL) as pu_is_null,\n",
    "        COUNTIF(dropoff_location_id IS NULL) as do_is_null,\n",
    "        \n",
    "        -- 2. Check auf 'Unknown' IDs (264=NV, 265=NA)\n",
    "        -- Das ist bei FHV oft der Fall!\n",
    "        COUNTIF(pickup_location_id IN (264, 265)) as pu_is_unknown,\n",
    "        COUNTIF(dropoff_location_id IN (264, 265)) as do_is_unknown,\n",
    "        \n",
    "        -- Prozentualer Anteil (NULL + Unknown)\n",
    "        ROUND((COUNTIF(pickup_location_id IS NULL OR pickup_location_id IN (264, 265)) / COUNT(*)) * 100, 1) as pct_bad_pickup\n",
    "        \n",
    "    FROM `{PROJECT_ID}.dimensional.Fact_Trips`\n",
    "    GROUP BY source_system\n",
    "    ORDER BY source_system\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        df = client.query(query).to_dataframe()\n",
    "        print(df.to_string(index=False))\n",
    "        \n",
    "        # Kurze Analyse f√ºr dich ausgeben\n",
    "        print(\"\\n--- ANALYSE ---\")\n",
    "        fhv_row = df[df['source_system'] == 'FHV']\n",
    "        if not fhv_row.empty:\n",
    "            missing_pct = fhv_row.iloc[0]['pct_bad_pickup']\n",
    "            if missing_pct > 50:\n",
    "                print(f\"‚ö†Ô∏è ACHTUNG: {missing_pct}% der FHV-Trips haben keine g√ºltige Location!\")\n",
    "                print(\"   -> Das ist normal f√ºr √§ltere FHV-Daten, schr√§nkt aber Heatmaps ein.\")\n",
    "            else:\n",
    "                print(f\"‚úÖ FHV Daten sehen gut aus ({missing_pct}% Missing).\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fehler bei der Abfrage: {e}\")\n",
    "\n",
    "check_fhv_locations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13a99c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïµÔ∏è‚Äç‚ôÇÔ∏è STARTE QUALIT√ÑTS-CHECK (Canonical Layer)...\n",
      "\n",
      "--- ERGEBNISSE ---\n",
      "                                  0\n",
      "total_rows                754144780\n",
      "null_pickups                      0\n",
      "null_fares                514630379\n",
      "credit_card_zero_revenue          0\n",
      "zero_distance_trips         2429564\n",
      "\n",
      "‚ùå WARNUNG: 514630379 kritische NULL-Werte gefunden!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lania/Documents/GitHub/bi_project_task2/.venv/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Zelle 8: Quality Audit (Der T√úV f√ºr deine Daten)\n",
    "def check_data_quality_stats():\n",
    "    print(\"üïµÔ∏è‚Äç‚ôÇÔ∏è STARTE QUALIT√ÑTS-CHECK (Canonical Layer)...\")\n",
    "    \n",
    "    sql_audit = f\"\"\"\n",
    "    SELECT \n",
    "        -- 1. Wie viele Fahrten haben wir insgesamt?\n",
    "        COUNT(*) as total_rows,\n",
    "    \n",
    "        -- 4. NULL CHECK (Darf eigentlich nicht sein, au√üer bei optionalen Feldern)\n",
    "        COUNTIF(pickup_datetime IS NULL) as null_pickups,\n",
    "        COUNTIF(fare_amount IS NULL) as null_fares,\n",
    "        \n",
    "        -- 5. LOGIK CHECK: Credit Card mit 0$ (Dein Sorgenkind)\n",
    "        COUNTIF(payment_type = 1 AND total_amount = 0) as credit_card_zero_revenue,\n",
    "        \n",
    "        -- 6. GEISTER-FAHRTEN (Keine Distanz)\n",
    "        COUNTIF(trip_distance = 0) as zero_distance_trips\n",
    "\n",
    "    FROM `{PROJECT_ID}.{CAN_DATASET}.canonical_unified_taxi`\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        df = client.query(sql_audit).to_dataframe()\n",
    "        print(\"\\n--- ERGEBNISSE ---\")\n",
    "        print(df.T) # Transponieren f√ºr bessere Lesbarkeit\n",
    "        \n",
    "        # Automatische Bewertung\n",
    "        null_errors = df['null_fares'][0] + df['null_pickups'][0]\n",
    "        if null_errors == 0:\n",
    "            print(\"\\n‚úÖ TEST BESTANDEN: Keine technischen NULL-Werte in kritischen Spalten.\")\n",
    "        else:\n",
    "            print(f\"\\n‚ùå WARNUNG: {null_errors} kritische NULL-Werte gefunden!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fehler beim Audit: {e}\")\n",
    "\n",
    "check_data_quality_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b6c01c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  source_system  total_fahrten  fahrten_ohne_preis  prozent_ohne_preis\n",
      "0        YELLOW      172066569                   0                 0.0\n",
      "1         GREEN       67447832                   0                 0.0\n",
      "2           FHV      514630379           514630379               100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lania/Documents/GitHub/bi_project_task2/.venv/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Zelle 9: Der \"Panic-Check\" - Woher kommen die NULLs?\n",
    "sql_debug = f\"\"\"\n",
    "SELECT \n",
    "    source_system,\n",
    "    COUNT(*) as total_fahrten,\n",
    "    COUNTIF(fare_amount IS NULL) as fahrten_ohne_preis,\n",
    "    ROUND(COUNTIF(fare_amount IS NULL) / COUNT(*) * 100, 2) as prozent_ohne_preis\n",
    "FROM `{PROJECT_ID}.{CAN_DATASET}.canonical_unified_taxi`\n",
    "GROUP BY source_system\n",
    "\"\"\"\n",
    "print(client.query(sql_debug).to_dataframe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "944f0bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ Erstelle die finale Gold-View f√ºr das BI-Reporting...\n",
      "‚úÖ Gold-View 'view_taxi_bi_final' wurde erstellt.\n",
      "üöÄ Dein Projekt ist nun bereit f√ºr die Visualisierung!\n"
     ]
    }
   ],
   "source": [
    "def create_final_gold_view():\n",
    "    print(\"üèÜ Erstelle die finale Gold-View f√ºr das BI-Reporting...\")\n",
    "    \n",
    "    view_sql = f\"\"\"\n",
    "    CREATE OR REPLACE VIEW `{PROJECT_ID}.dimensional.view_taxi_bi_final` AS\n",
    "    SELECT \n",
    "        -- Zeit & IDs\n",
    "        f.trip_id,\n",
    "        f.pickup_datetime,\n",
    "        f.dropoff_datetime,\n",
    "        f.pickup_date_key,\n",
    "        \n",
    "        -- Dimensionen: Wer & Wie?\n",
    "        v.vendor_name,\n",
    "        p.payment_description as payment_method,\n",
    "        r.rate_description as rate_type,\n",
    "        t.trip_description as trip_category,\n",
    "        \n",
    "        -- Dimensionen: Wo?\n",
    "        loc_pu.borough as pickup_borough,\n",
    "        loc_pu.zone as pickup_zone,\n",
    "        loc_do.borough as dropoff_borough,\n",
    "        loc_do.zone as dropoff_zone,\n",
    "        \n",
    "        -- Shared Ride Info\n",
    "        s.sr_description as ride_type,\n",
    "        \n",
    "        -- Kennzahlen (Measures)\n",
    "        f.passenger_count,\n",
    "        f.trip_distance,\n",
    "        f.fare_amount,\n",
    "        f.tip_amount,\n",
    "        f.total_amount,\n",
    "        f.duration_minutes,\n",
    "        \n",
    "        -- Qualit√§tssicherung\n",
    "        f.source_system,\n",
    "        f.dq_issue_flag\n",
    "        \n",
    "    FROM `{PROJECT_ID}.dimensional.Fact_Trips` f\n",
    "    LEFT JOIN `{PROJECT_ID}.dimensional.dim_vendor` v ON f.vendor_id = v.vendor_id\n",
    "    LEFT JOIN `{PROJECT_ID}.dimensional.dim_payment_type` p ON f.payment_type_id = p.payment_type_id\n",
    "    LEFT JOIN `{PROJECT_ID}.dimensional.dim_rate_code` r ON f.rate_code_id = r.rate_code_id\n",
    "    LEFT JOIN `{PROJECT_ID}.dimensional.dim_trip_type` t ON f.trip_type_id = t.trip_type_id\n",
    "    LEFT JOIN `{PROJECT_ID}.dimensional.dim_shared_ride` s ON f.sr_flag = s.sr_flag\n",
    "    LEFT JOIN `{PROJECT_ID}.dimensional.dim_location` loc_pu ON f.pickup_location_id = loc_pu.location_id\n",
    "    LEFT JOIN `{PROJECT_ID}.dimensional.dim_location` loc_do ON f.dropoff_location_id = loc_do.location_id\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        client.query(view_sql).result()\n",
    "        print(\"‚úÖ Gold-View 'view_taxi_bi_final' wurde erstellt.\")\n",
    "        print(\"üöÄ Dein Projekt ist nun bereit f√ºr die Visualisierung!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fehler bei der View-Erstellung: {e}\")\n",
    "\n",
    "create_final_gold_view()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

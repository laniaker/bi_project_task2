{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d77b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sorgt daf√ºr, dass Plots im Notebook angezeigt werden\n",
    "%matplotlib inline\n",
    "\n",
    "# Lesbarkeit in der Exploration erh√∂hen\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "pd.set_option(\"display.width\", 140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c93bf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ QUELLE GEFUNDEN: Dataset 'dimensional' liegt in Region: 'EU'\n"
     ]
    }
   ],
   "source": [
    "# Zelle 1 & 2: Setup mit automatischer Regionen-Korrektur\n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import NotFound\n",
    "import logging\n",
    "import uuid\n",
    "import datetime\n",
    "\n",
    "# Logging Setup\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# --- CONFIG ---\n",
    "PROJECT_ID = \"taxi-bi-project\" # Deine ID aus dem Log\n",
    "DIM_DATASET = \"dimensional\"\n",
    "SOURCE_DATASET = \"staging\"      \n",
    "CAN_DATASET = \"canonical\"   \n",
    "\n",
    "# Tabellen\n",
    "TARGET_TABLE = \"dimensional\"\n",
    "\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "try:\n",
    "    src_ds_ref = client.get_dataset(f\"{PROJECT_ID}.{DIM_DATASET}\")\n",
    "    CORRECT_LOCATION = src_ds_ref.location\n",
    "    print(f\"‚úÖ QUELLE GEFUNDEN: Dataset '{DIM_DATASET}' liegt in Region: '{CORRECT_LOCATION}'\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå KRITISCHER FEHLER: Konnte Quell-Dataset '{DIM_DATASET}' nicht finden!\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0abdb223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tabelle erfolgreich erstellt: taxi-bi-project.dimensional.ref_black_car\n",
      "   Bereinigte Spalten: ['LICENSEE_NUMBER', 'NAME_OF_LICENSEE', 'ALTERNATE_NAME_OF_LICENSEE']...\n",
      "‚úÖ Tabelle erfolgreich erstellt: taxi-bi-project.dimensional.ref_luxury_limo\n",
      "   Bereinigte Spalten: ['LICENSEE_NUMBER', 'NAME_OF_LICENSEE', 'ALTERNATE_NAME_OF_LICENSEE']...\n",
      "‚úÖ Tabelle erfolgreich erstellt: taxi-bi-project.dimensional.ref_community_car\n",
      "   Bereinigte Spalten: ['LICENSEE_NUMBER', 'NAME_OF_LICENSEE', 'ALTERNATE_NAME_OF_LICENSEE']...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "import re\n",
    "\n",
    "def upload_reference_csvs():\n",
    "    files_to_upload = {\n",
    "        \"ref_black_car\": \"/Users/lania/Documents/GitHub/bi_project_task2/notebook_dimensional/current_black_car_bases.csv\",\n",
    "        \"ref_luxury_limo\": \"/Users/lania/Documents/GitHub/bi_project_task2/notebook_dimensional/current_luxury_limousine_bases.csv\",\n",
    "        \"ref_community_car\": \"/Users/lania/Documents/GitHub/bi_project_task2/notebook_dimensional/current_community_car_service_bases.csv\"\n",
    "    }\n",
    "\n",
    "    for table_name, file_path in files_to_upload.items():\n",
    "        try:\n",
    "            # 1. Datei laden mit utf-8-sig um das unsichtbare BOM-Zeichen zu entfernen\n",
    "            df = pd.read_csv(\n",
    "                file_path, \n",
    "                sep=None, \n",
    "                quotechar=\"'\", \n",
    "                engine='python', \n",
    "                encoding='utf-8-sig', # <--- WICHTIG: Entfernt das Ôªø Zeichen\n",
    "                on_bad_lines='warn'\n",
    "            )\n",
    "            \n",
    "            # Daten von restlichen Hochkommas befreien\n",
    "            for col in df.select_dtypes(include=['object']):\n",
    "                df[col] = df[col].str.strip(\"'\")\n",
    "\n",
    "            # 2. SPALTEN REINIGEN (Extrem sicher)\n",
    "            # Wir behalten nur Buchstaben, Zahlen und Unterstriche. \n",
    "            # Alles andere (inkl. unsichtbarer Zeichen) wird entfernt.\n",
    "            new_columns = []\n",
    "            for c in df.columns:\n",
    "                # Ersetze Leerzeichen/Bindestriche durch Unterstrich\n",
    "                clean_name = c.replace(' ', '_').replace('-', '_').replace('/', '_')\n",
    "                # Entferne alle Zeichen, die nicht A-Z, 0-9 oder _ sind\n",
    "                clean_name = re.sub(r'[^a-zA-Z0-9_]', '', clean_name)\n",
    "                # BigQuery Spalten d√ºrfen nicht mit einer Zahl beginnen\n",
    "                if clean_name[0].isdigit():\n",
    "                    clean_name = 'f_' + clean_name\n",
    "                new_columns.append(clean_name)\n",
    "            \n",
    "            df.columns = new_columns\n",
    "            \n",
    "            # 3. Upload nach BigQuery\n",
    "            table_id = f\"{PROJECT_ID}.{DIM_DATASET}.{table_name}\"\n",
    "            \n",
    "            job_config = bigquery.LoadJobConfig(\n",
    "                write_disposition=\"WRITE_TRUNCATE\",\n",
    "                autodetect=True\n",
    "            )\n",
    "            \n",
    "            client.load_table_from_dataframe(df, table_id, job_config=job_config).result()\n",
    "            \n",
    "            print(f\"‚úÖ Tabelle erfolgreich erstellt: {table_id}\")\n",
    "            print(f\"   Bereinigte Spalten: {list(df.columns[:3])}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Fehler beim Hochladen von {file_path}:\")\n",
    "            print(f\"   Details: {e}\")\n",
    "\n",
    "# Ausf√ºhren\n",
    "upload_reference_csvs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5911943a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Erstelle Time Dimension: taxi-bi-project.dimensional.dim_date ---\n",
      "‚úÖ dim_date erfolgreich erstellt (2015-2026).\n"
     ]
    }
   ],
   "source": [
    "def create_dim_date():\n",
    "    DIM_DATASET = \"dimensional\"\n",
    "    dim_table_id = f\"{PROJECT_ID}.{DIM_DATASET}.dim_date\"\n",
    "    \n",
    "    print(f\"--- Erstelle Time Dimension: {dim_table_id} ---\")\n",
    "    \n",
    "    # Wir generieren Daten von 2015 bis Ende 2026\n",
    "    # Uhrzeit - Stunde \n",
    "    sql_date = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `taxi-bi-project.dimensional.dim_datetime` AS\n",
    "    SELECT\n",
    "        -- Generiert einen Timestamp f√ºr jede Stunde\n",
    "        stunde AS datetime_key,\n",
    "\n",
    "        EXTRACT(YEAR FROM stunde) AS year,\n",
    "        EXTRACT(MONTH FROM stunde) AS month,\n",
    "        FORMAT_TIMESTAMP('%B', stunde) AS month_name, \n",
    "        EXTRACT(QUARTER FROM stunde) AS quarter,\n",
    "        EXTRACT(DAYOFWEEK FROM stunde) AS day_of_week,\n",
    "        -- Hier ist die neue Stunden-Spalte (0-23)\n",
    "        EXTRACT(HOUR FROM stunde) AS hour,\n",
    "        CASE WHEN EXTRACT(DAYOFWEEK FROM stunde) IN (1, 7) THEN TRUE ELSE FALSE END AS is_weekend\n",
    "    FROM \n",
    "        UNNEST(\n",
    "            GENERATE_TIMESTAMP_ARRAY(\n",
    "                '2010-01-01 00:00:00', \n",
    "                '2025-12-31 23:00:00', \n",
    "                INTERVAL 1 HOUR\n",
    "            )\n",
    "        ) AS stunde;\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        client.query(sql_date).result()\n",
    "        print(\"‚úÖ dim_date erfolgreich erstellt (2015-2026).\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fehler bei dim_date: {e}\")\n",
    "\n",
    "# Ausf√ºhren\n",
    "create_dim_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df77383e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Erstelle Layer 3 (Dimensionen) in 'dimensional' ---\n",
      "‚úÖ Alle Dimensionstabellen (Vendor, Payment, Rate, Type, Shared, Location) wurden erstellt.\n"
     ]
    }
   ],
   "source": [
    "def create_fact_layer_v3():\n",
    "    DIM_DATASET = \"dimensional\"\n",
    "    CAN_DATASET = \"canonical\"\n",
    "    \n",
    "    # Pfade definieren\n",
    "    source_table = f\"{PROJECT_ID}.{CAN_DATASET}.canonical_unified_taxi\"\n",
    "    fact_table = f\"{PROJECT_ID}.{DIM_DATASET}.Fact_Trips\"\n",
    "    dim_base_table = f\"{PROJECT_ID}.{DIM_DATASET}.dim_base\"\n",
    "\n",
    "    print(f\"--- Starte Layer-Optimierung: {fact_table} ---\")\n",
    "\n",
    "    # 1. SCHRITT: Erstellung der dim_base (dein SQL)\n",
    "    sql_dim_base = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{dim_base_table}` AS\n",
    "    SELECT\n",
    "        LICENSEE_NUMBER AS base_number,\n",
    "        NAME_OF_LICENSEE AS base_name,\n",
    "        base_type,\n",
    "        TRUE AS is_currently_active\n",
    "    FROM (\n",
    "        SELECT LICENSEE_NUMBER, NAME_OF_LICENSEE, 'Black Car' as base_type FROM `{PROJECT_ID}.{DIM_DATASET}.ref_black_car`\n",
    "        UNION ALL\n",
    "        SELECT LICENSEE_NUMBER, NAME_OF_LICENSEE, 'Luxury Limo' FROM `{PROJECT_ID}.{DIM_DATASET}.ref_luxury_limo`\n",
    "        UNION ALL\n",
    "        SELECT LICENSEE_NUMBER, NAME_OF_LICENSEE, 'Community Car' FROM `{PROJECT_ID}.{DIM_DATASET}.ref_community_car`\n",
    "    );\n",
    "    \"\"\"\n",
    "\n",
    "    # 2. SCHRITT: Fact Table mit JOIN zur dim_base\n",
    "    sql_fact = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{fact_table}`\n",
    "    PARTITION BY DATETIME_TRUNC(pickup_datetime, MONTH)\n",
    "    CLUSTER BY base_type, vendor_id\n",
    "    AS\n",
    "    SELECT\n",
    "        -- IDs & Links\n",
    "        t.trip_id,\n",
    "        t.vendor_id,\n",
    "        t.dispatching_base_nummer,\n",
    "        \n",
    "        -- Mapping-Daten aus der dim_base\n",
    "        IFNULL(b.base_name, 'Unknown') AS base_name,\n",
    "        IFNULL(b.base_type, 'Unknown') AS base_type,\n",
    "        IFNULL(b.is_currently_active, FALSE) AS is_active_base,\n",
    "        \n",
    "        -- DATE & TIME\n",
    "        DATE(t.pickup_datetime) AS pickup_date_key, \n",
    "        t.pickup_datetime,\n",
    "        t.dropoff_datetime,\n",
    "        \n",
    "        -- Locations\n",
    "        COALESCE(t.pickup_location_id, 263) AS pickup_location_id, \n",
    "        COALESCE(t.dropoff_location_id, 263) AS dropoff_location_id,\n",
    "        \n",
    "        -- Payment & Rate\n",
    "        IFNULL(t.payment_type, 0) AS payment_type_id,\n",
    "        IFNULL(t.RatecodeID, 99) AS rate_code_id,\n",
    "        \n",
    "        -- System Info\n",
    "        t.source_system,\n",
    "        t.dq_issue_flag,\n",
    "        \n",
    "        -- Measures\n",
    "        COALESCE(t.passenger_count, 0) AS passenger_count,\n",
    "        COALESCE(t.trip_distance, 0) AS trip_distance,\n",
    "        COALESCE(t.total_amount, 0) AS total_amount,\n",
    "        \n",
    "        -- Berechnete Measures\n",
    "        TIMESTAMP_DIFF(t.dropoff_datetime, t.pickup_datetime, MINUTE) AS duration_minutes\n",
    "\n",
    "    FROM `{source_table}` t\n",
    "    -- Verkn√ºpfung zur neuen Dimension\n",
    "    LEFT JOIN `{dim_base_table}` b \n",
    "      ON t.dispatching_base_nummer = b.base_number\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"üõ†Ô∏è Erstelle Dimension: dim_base...\")\n",
    "        client.query(sql_dim_base).result()\n",
    "        \n",
    "        print(\"üöÄ Erstelle Fact Table: Fact_Trips...\")\n",
    "        client.query(sql_fact).result()\n",
    "        \n",
    "        print(f\"‚úÖ Erfolg! Fact_Trips ist nun mit dim_base gemappt.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fehler bei der Verarbeitung: {e}\")\n",
    "\n",
    "# Ausf√ºhren\n",
    "create_fact_layer_v3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7867a47b",
   "metadata": {},
   "source": [
    "** Create Dimensional **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef9032ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üîç QA CHECK: Missing Location IDs by System ---\n",
      "source_system  total_trips  pu_is_null  do_is_null  pu_is_unknown  do_is_unknown  pct_bad_pickup\n",
      "          FHV    510080635           0           0      148848097       99316788            29.2\n",
      "        GREEN     67448057           0           0         115970         250756             0.2\n",
      "       YELLOW    172064982           0           0        2748871        3007044             1.6\n",
      "\n",
      "--- ANALYSE ---\n",
      "‚úÖ FHV Daten sehen gut aus (29.2% Missing).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lania/Documents/GitHub/bi_project_task2/.venv/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Zelle 7: QA Check - Sind FHV Locations da?\n",
    "def check_fhv_locations():\n",
    "    print(\"--- üîç QA CHECK: Missing Location IDs by System ---\")\n",
    "    \n",
    "    # Wir pr√ºfen Fact_Trips, da dies die Basis f√ºr deine Analysen ist\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        source_system,\n",
    "        COUNT(*) as total_trips,\n",
    "        \n",
    "        -- 1. Check auf ECHTE NULL-Werte\n",
    "        COUNTIF(pickup_location_id IS NULL) as pu_is_null,\n",
    "        COUNTIF(dropoff_location_id IS NULL) as do_is_null,\n",
    "        \n",
    "        -- 2. Check auf 'Unknown' IDs (264=NV, 265=NA)\n",
    "        -- Das ist bei FHV oft der Fall!\n",
    "        COUNTIF(pickup_location_id IN (264, 265)) as pu_is_unknown,\n",
    "        COUNTIF(dropoff_location_id IN (264, 265)) as do_is_unknown,\n",
    "        \n",
    "        -- Prozentualer Anteil (NULL + Unknown)\n",
    "        ROUND((COUNTIF(pickup_location_id IS NULL OR pickup_location_id IN (264, 265)) / COUNT(*)) * 100, 1) as pct_bad_pickup\n",
    "        \n",
    "    FROM `{PROJECT_ID}.dimensional.Fact_Trips`\n",
    "    GROUP BY source_system\n",
    "    ORDER BY source_system\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        df = client.query(query).to_dataframe()\n",
    "        print(df.to_string(index=False))\n",
    "        \n",
    "        # Kurze Analyse f√ºr dich ausgeben\n",
    "        print(\"\\n--- ANALYSE ---\")\n",
    "        fhv_row = df[df['source_system'] == 'FHV']\n",
    "        if not fhv_row.empty:\n",
    "            missing_pct = fhv_row.iloc[0]['pct_bad_pickup']\n",
    "            if missing_pct > 50:\n",
    "                print(f\"‚ö†Ô∏è ACHTUNG: {missing_pct}% der FHV-Trips haben keine g√ºltige Location!\")\n",
    "                print(\"   -> Das ist normal f√ºr √§ltere FHV-Daten, schr√§nkt aber Heatmaps ein.\")\n",
    "            else:\n",
    "                print(f\"‚úÖ FHV Daten sehen gut aus ({missing_pct}% Missing).\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fehler bei der Abfrage: {e}\")\n",
    "\n",
    "check_fhv_locations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "944f0bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üõ†Ô∏è Repariere Fact Table: taxi-bi-project.dimensional.Fact_Trips ---\n",
      "üóëÔ∏è L√∂sche alte Tabelle taxi-bi-project.dimensional.Fact_Trips...\n",
      "üöÄ Erstelle Fact Table neu (inkl. trip_type_id)...\n",
      "‚úÖ Fact_Trips ist jetzt vollst√§ndig.\n"
     ]
    }
   ],
   "source": [
    "def create_fact_layer():\n",
    "    DIM_DATASET = \"dimensional\"\n",
    "    CAN_DATASET = \"canonical\"\n",
    "    \n",
    "    source_table = f\"{PROJECT_ID}.{CAN_DATASET}.canonical_unified_taxi\"\n",
    "    fact_table = f\"{PROJECT_ID}.{DIM_DATASET}.Fact_Trips\"\n",
    "    dim_base_table = f\"{PROJECT_ID}.{DIM_DATASET}.dim_base\"\n",
    "\n",
    "    print(f\"--- üõ†Ô∏è Repariere Fact Table: {fact_table} ---\")\n",
    "\n",
    "    sql_fact = f\"\"\"\n",
    "    CREATE TABLE `{fact_table}`\n",
    "    PARTITION BY DATETIME_TRUNC(pickup_datetime, MONTH)\n",
    "    CLUSTER BY base_type, vendor_id\n",
    "    AS\n",
    "    SELECT\n",
    "        t.trip_id,\n",
    "        t.vendor_id,\n",
    "        t.dispatching_base_nummer,\n",
    "        \n",
    "        -- Mapping-Daten aus der dim_base (direkt in die Fact f√ºr Performance)\n",
    "        IFNULL(b.base_name, 'Unknown') AS base_name,\n",
    "        IFNULL(b.base_type, 'Unknown') AS base_type,\n",
    "        IFNULL(b.is_currently_active, FALSE) AS is_active_base,\n",
    "        \n",
    "        -- DATE & TIME\n",
    "        DATE(t.pickup_datetime) AS pickup_date_key, \n",
    "        t.pickup_datetime,\n",
    "        t.dropoff_datetime,\n",
    "        \n",
    "        -- Locations\n",
    "        COALESCE(t.pickup_location_id, 263) AS pickup_location_id, \n",
    "        COALESCE(t.dropoff_location_id, 263) AS dropoff_location_id,\n",
    "        \n",
    "        -- Payment, Rate & TYPES (WICHTIG: Diese haben in der View gefehlt!)\n",
    "        IFNULL(t.payment_type, 0) AS payment_type_id,\n",
    "        IFNULL(t.RatecodeID, 99) AS rate_code_id,\n",
    "        t.Trip_type AS trip_type_id,           -- <--- Wieder hinzugef√ºgt\n",
    "        IFNULL(t.SR_Flag, FALSE) AS sr_flag,   -- <--- Wieder hinzugef√ºgt\n",
    "        \n",
    "        -- Measures\n",
    "        COALESCE(t.passenger_count, 0) AS passenger_count,\n",
    "        COALESCE(t.trip_distance, 0) AS trip_distance,\n",
    "        COALESCE(t.total_amount, 0) AS total_amount,\n",
    "        COALESCE(t.fare_amount, 0) AS fare_amount,\n",
    "        COALESCE(t.tip_amount, 0) AS tip_amount,\n",
    "        \n",
    "        -- System & Berechnung\n",
    "        t.source_system,\n",
    "        t.dq_issue_flag,\n",
    "        TIMESTAMP_DIFF(t.dropoff_datetime, t.pickup_datetime, MINUTE) AS duration_minutes\n",
    "    FROM `{source_table}` t\n",
    "    LEFT JOIN `{dim_base_table}` b \n",
    "      ON t.dispatching_base_nummer = b.base_number\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"üóëÔ∏è L√∂sche alte Tabelle {fact_table}...\")\n",
    "        client.query(f\"DROP TABLE IF EXISTS `{fact_table}`\").result()\n",
    "        \n",
    "        print(\"üöÄ Erstelle Fact Table neu (inkl. trip_type_id)...\")\n",
    "        client.query(sql_fact).result()\n",
    "        print(f\"‚úÖ Fact_Trips ist jetzt vollst√§ndig.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fehler: {e}\")\n",
    "\n",
    "create_fact_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e0bf9ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ Erstelle die finale Gold-View...\n",
      "‚úÖ Gold-View 'view_taxi_bi_final' erfolgreich erstellt!\n"
     ]
    }
   ],
   "source": [
    "def create_final_gold_view_v3():\n",
    "    print(\"üèÜ Erstelle die finale Gold-View...\")\n",
    "    \n",
    "    view_sql = f\"\"\"\n",
    "    CREATE OR REPLACE VIEW `{PROJECT_ID}.dimensional.view_taxi_bi_final` AS\n",
    "    SELECT \n",
    "        f.trip_id,\n",
    "        f.pickup_datetime,\n",
    "        f.dropoff_datetime,\n",
    "        f.pickup_date_key,\n",
    "        \n",
    "        -- Basis Info\n",
    "        f.dispatching_base_nummer as base_id,\n",
    "        f.base_name,\n",
    "        f.base_type,\n",
    "        f.is_active_base,\n",
    "        \n",
    "        -- Dimensionen Joins\n",
    "        v.vendor_name,\n",
    "        p.payment_description as payment_method,\n",
    "        r.rate_description as rate_type,\n",
    "        t.trip_description as trip_category,\n",
    "        s.sr_description as ride_type,\n",
    "        \n",
    "        -- Locations\n",
    "        loc_pu.borough as pickup_borough,\n",
    "        loc_pu.zone as pickup_zone,\n",
    "        loc_do.borough as dropoff_borough,\n",
    "        loc_do.zone as dropoff_zone,\n",
    "        \n",
    "        -- Kennzahlen\n",
    "        f.passenger_count,\n",
    "        f.trip_distance,\n",
    "        f.fare_amount,\n",
    "        f.tip_amount,\n",
    "        f.total_amount,\n",
    "        f.duration_minutes,\n",
    "        \n",
    "        f.source_system,\n",
    "        f.dq_issue_flag\n",
    "        \n",
    "    FROM `{PROJECT_ID}.dimensional.Fact_Trips` f\n",
    "    LEFT JOIN `{PROJECT_ID}.dimensional.dim_vendor` v ON f.vendor_id = v.vendor_id\n",
    "    LEFT JOIN `{PROJECT_ID}.dimensional.dim_payment_type` p ON f.payment_type_id = p.payment_type_id\n",
    "    LEFT JOIN `{PROJECT_ID}.dimensional.dim_rate_code` r ON f.rate_code_id = r.rate_code_id\n",
    "    LEFT JOIN `{PROJECT_ID}.dimensional.dim_trip_type` t ON f.trip_type_id = t.trip_type_id\n",
    "    LEFT JOIN `{PROJECT_ID}.dimensional.dim_shared_ride` s ON f.sr_flag = s.sr_flag\n",
    "    LEFT JOIN `{PROJECT_ID}.dimensional.dim_location` loc_pu ON f.pickup_location_id = loc_pu.location_id\n",
    "    LEFT JOIN `{PROJECT_ID}.dimensional.dim_location` loc_do ON f.dropoff_location_id = loc_do.location_id\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        client.query(view_sql).result()\n",
    "        print(\"‚úÖ Gold-View 'view_taxi_bi_final' erfolgreich erstellt!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fehler: {e}\")\n",
    "\n",
    "create_final_gold_view_v3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1b6c01c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  source_system  total_fahrten  fahrten_ohne_preis  prozent_ohne_preis\n",
      "0        YELLOW      172064982                   0                 0.0\n",
      "1         GREEN       67448057                   0                 0.0\n",
      "2           FHV      510080635           510080635               100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lania/Documents/GitHub/bi_project_task2/.venv/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Zelle 9: Der \"Panic-Check\" - Woher kommen die NULLs?\n",
    "sql_debug = f\"\"\"\n",
    "SELECT \n",
    "    source_system,\n",
    "    COUNT(*) as total_fahrten,\n",
    "    COUNTIF(fare_amount IS NULL) as fahrten_ohne_preis,\n",
    "    ROUND(COUNTIF(fare_amount IS NULL) / COUNT(*) * 100, 2) as prozent_ohne_preis\n",
    "FROM `{PROJECT_ID}.{CAN_DATASET}.canonical_unified_taxi`\n",
    "GROUP BY source_system\n",
    "\"\"\"\n",
    "print(client.query(sql_debug).to_dataframe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
